# 

- Hadoop3.x





# å¤§æ•°æ®æ¦‚è®º

## æ¦‚å¿µ

å¤§æ•°æ®ï¼ˆBig Dataï¼‰ï¼šæŒ‡æ— æ³•åœ¨ä¸€å®šæ—¶é—´èŒƒå›´å†…ç”¨å¸¸è§„è½¯ä»¶å·¥å…·è¿›è¡Œæ•æ‰ã€ç®¡ç†å’Œå¤„ç†çš„æ•°æ®é›†åˆï¼Œæ˜¯éœ€è¦æ–°å¤„ç†æ¨¡å¼æ‰èƒ½å…·æœ‰æ›´å¼ºçš„å†³ç­–åŠ›ã€æ´å¯Ÿå‘ç°åŠ›å’Œæµç¨‹ä¼˜åŒ–èƒ½åŠ›çš„æµ·é‡ã€é«˜å¢é•¿ç‡å’Œå¤šæ ·åŒ–çš„ä¿¡æ¯èµ„äº§ã€‚

å¤§æ•°æ®ä¸»è¦è§£å†³ï¼Œ**æµ·é‡**æ•°æ®çš„**é‡‡é›†ã€å­˜å‚¨å’Œåˆ†æè®¡ç®—**é—®é¢˜ã€‚

![](https://notes2021.oss-cn-beijing.aliyuncs.com/2021/image-20220803215533752.png)



## ç‰¹ç‚¹ 4V

- å¤§é‡ Volume
  - æˆªè‡³ç›®å‰ï¼Œäººç±»ç”Ÿäº§çš„æ‰€æœ‰å°åˆ·ææ–™çš„æ•°æ®é‡æ˜¯200PBï¼Œè€Œå†å²ä¸Šå…¨äººç±»æ€»å…±è¯´è¿‡çš„è¯çš„æ•°æ®é‡å¤§çº¦æ˜¯5EBã€‚å½“å‰ï¼Œå…¸å‹ä¸ªäººè®¡ç®—æœºç¡¬ç›˜çš„å®¹é‡ä¸ºTBé‡çº§ï¼Œè€Œä¸€äº›å¤§ä¼ä¸šçš„æ•°æ®é‡å·²ç»æ¥è¿‘EBé‡çº§ã€‚
- é«˜é€Ÿ Velocity
  - å¤§æ•°æ®åŒºåˆ†äºä¼ ç»Ÿæ•°æ®æŒ–æ˜çš„æœ€æ˜¾è‘—ç‰¹å¾ã€‚æµ·é‡æ•°æ®é¢å‰ï¼Œå¤„ç†æ•°æ®çš„æ•ˆç‡å°±æ˜¯ä¼ä¸šçš„ç”Ÿå‘½ã€‚
- å¤šæ · Variety
  - åˆ†ä¸ºç»“æ„åŒ–æ•°æ®å’Œéç»“æ„åŒ–æ•°æ®ã€‚
  - éç»“æ„åŒ–æ•°æ®è¶Šæ¥è¶Šå¤šï¼ŒåŒ…æ‹¬æ—¥å¿—ã€éŸ³é¢‘ã€è§†é¢‘ã€å›¾ç‰‡ã€åœ°ç†ä½ç½®ä¿¡æ¯ç­‰ï¼Œå¤šç±»å‹çš„æ•°æ®å¯¹æ•°æ®çš„å¤„ç†èƒ½åŠ›æå‡ºäº†æ›´é«˜è¦æ±‚ã€‚
- ä½ä»·å€¼å¯†åº¦ Value
  - ä»·å€¼å¯†åº¦çš„é«˜ä½ä¸æ•°æ®æ€»é‡çš„å¤§å°æˆåæ¯”ã€‚
  - å¦‚ä½•å¿«é€Ÿå¯¹æœ‰ä»·å€¼æ•°æ®â€æçº¯â€œæˆä¸ºç›®å‰å¤§æ•°æ®èƒŒæ™¯ä¸‹å¾…è§£å†³çš„éš¾é¢˜ã€‚



## åº”ç”¨åœºæ™¯

- æŠ–éŸ³ï¼šæ¨èä½ å–œæ¬¢çš„è§†é¢‘
- ç”µå•†ï¼šå¹¿å‘Šæ¨èï¼Œç»™ç”¨æˆ·æ¨èå¯èƒ½å–œæ¬¢çš„äº§å“
- é›¶å”®ï¼šåˆ†æç”¨æˆ·æ¶ˆè´¹ä¹ æƒ¯ï¼Œä¸ºç”¨æˆ·è´­ä¹°å•†å“æä¾›æ–¹ä¾¿ï¼Œä»è€Œæå‡å•†å“é”€é‡ã€‚ç»å…¸æ¡ˆä¾‹ï¼Œçº¸å°¿å¸ƒ + å•¤é…’ã€‚
- ç‰©æµä»“å‚¨ï¼šäº¬ä¸œç‰©æµ
- ä¿é™©ï¼šæµ·é‡æ•°æ®æŒ–æ˜åŠé£é™©é¢„æµ‹ï¼Œç²¾å‡†è¥é”€ï¼Œæå‡ç²¾ç»†åŒ–å®šä»·èƒ½åŠ›ã€‚
- é‡‘èï¼šå¸®åŠ©é‡‘èæœºæ„æ¨èä¼˜è´¨å®¢æˆ·
- æˆ¿äº§ï¼šç²¾å‡†æ¨æµ‹ä¸è¥é”€
- äººå·¥æ™ºèƒ½ + 5G + ç‰©è”ç½‘ + è™šæ‹Ÿä¸ç°å®

## å‘å±•å‰æ™¯





## å¤§æ•°æ®éƒ¨é—¨é—´ä¸šåŠ¡æµç¨‹åˆ†æ

![](https://notes2021.oss-cn-beijing.aliyuncs.com/2021/image-20220804075651997.png)











## å¤§æ•°æ®éƒ¨é—¨å†…ç»„ç»‡æ¶æ„





![](https://notes2021.oss-cn-beijing.aliyuncs.com/2021/image-20220803221504164.png)









# Hadoopå…¥é—¨

- è¯¾ç¨‹å‡çº§å†…å®¹
  - yarn
  - ç”Ÿäº§è°ƒä¼˜æ‰‹å†Œ
  - æºç 
- è¯¾ç¨‹ç‰¹è‰²
  - æ–° hadoop 3.1.3
  - ç»† ä»æ­å»ºé›†ç¾¤å¼€å§‹ æ¯ä¸€ä¸ªé…ç½® æ¯ä¸€è¡Œä»£ç éƒ½æœ‰æ³¨é‡Š
  - çœŸ 20+ä¼ä¸šæ¡ˆä¾‹ 30+ä¼ä¸šè°ƒä¼˜ ä»ç™¾ä¸‡ä»£ç ä¸­é˜…è¯»æºç 
  - å…¨ å…¨å¥—èµ„æ–™
- æŠ€æœ¯åŸºç¡€è¦æ±‚
  - JavaSE
  - maven
  - Idea
  - Linuxå¸¸ç”¨å‘½ä»¤



## 1 æ¦‚è¿°

### 1.1 æ˜¯ä»€ä¹ˆ

- åˆ†å¸ƒå¼ç³»ç»ŸåŸºç¡€ç»“æ„
- ä¸»è¦è§£å†³ï¼Œæµ·é‡æ•°æ®çš„å­˜å‚¨å’Œæµ·é‡æ•°æ®çš„åˆ†æè®¡ç®—é—®é¢˜
- å¹¿ä¹‰ä¸Šè®²ï¼ŒæŒ‡ä¸€ä¸ªæ›´å¹¿æ³›çš„æ¦‚å¿µâ€”â€”Hadoopç”Ÿæ€åœˆ

### 1.2 å‘å±•å†å²ï¼ˆäº†è§£ï¼‰

- å¯ä»¥è¯´Googleæ˜¯Hadoopçš„æ€æƒ³ä¹‹æºï¼ˆGoogleåœ¨å¤§æ•°æ®æ–¹é¢çš„ä¸‰ç¯‡è®ºæ–‡)
  - GFS--->HDFS
  - Map-Reduce --->MR
  - BigTable--->HBase



### 1.3 å‘è¡Œç‰ˆæœ¬ï¼ˆäº†è§£ï¼‰

- Apache
- Cloudera
- Hortonworks

### 1.4 ä¼˜åŠ¿ï¼ˆ4é«˜ï¼‰

- é«˜å¯é æ€§ï¼šåº•å±‚ç»´æŠ¤å¤šä¸ªæ•°æ®å‰¯æœ¬
- é«˜æ‰©å±•æ€§ï¼šåœ¨é›†ç¾¤é—´åˆ†é…ä»»åŠ¡æ•°æ®ï¼Œå¯æ–¹ä¾¿åœ°æ‰©å±•æ•°ä»¥åƒè®¡çš„ç»“ç‚¹

![](https://notes2021.oss-cn-beijing.aliyuncs.com/2021/image-20220804081554349.png)

- é«˜æ•ˆæ€§ï¼šå¹¶è¡Œå·¥ä½œçš„ï¼Œä»¥åŠ å¿«ä»»åŠ¡å¤„ç†é€Ÿåº¦

![](https://notes2021.oss-cn-beijing.aliyuncs.com/2021/image-20220804081642809.png)

- é«˜å®¹é”™æ€§ï¼šèƒ½å¤Ÿè‡ªåŠ¨å°†å¤±è´¥çš„ä»»åŠ¡é‡è¯•





### 1.5 ç»„æˆğŸˆï¼ˆé¢è¯•é‡ç‚¹ï¼‰

- `1.x`ã€`2.x`ã€`3.x`åŒºåˆ«

![](https://notes2021.oss-cn-beijing.aliyuncs.com/2021/image-20220804081750498.png)



#### HDFS æ¶æ„æ¦‚è¿°ï¼ˆåˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿï¼‰ï¼ˆå­˜å‚¨ï¼‰

- Hadoop Distributed File System
- è§£å†³å­˜å‚¨é—®é¢˜

![](https://notes2021.oss-cn-beijing.aliyuncs.com/2021/image-20220805223542512.png)



#### YARNï¼ˆèµ„æºç®¡ç†å™¨ï¼‰

- Yet Another Resource Negotiator
- Hadoopçš„èµ„æºç®¡ç†å™¨ï¼Œä¸»è¦ç®¡ç†CPUå’Œå†…å­˜

![](https://notes2021.oss-cn-beijing.aliyuncs.com/2021/image-20220804082125762.png)





#### MapReduce æ¶æ„æ¦‚è¿°ï¼ˆè®¡ç®—ï¼‰

MapReduceå°†è®¡ç®—è¿‡ç¨‹åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šMap å’Œ Reduce

- 1ï¼‰Map é˜¶æ®µå¹¶è¡Œå¤„ç†è¾“å…¥æ•°æ®

- 2ï¼‰Reduce é˜¶æ®µå¯¹Map ç»“æœè¿›è¡Œæ±‡æ€»



![](https://notes2021.oss-cn-beijing.aliyuncs.com/2021/image-20220804082341454.png)



#### ä¸‰è€…å…³ç³»



![](https://notes2021.oss-cn-beijing.aliyuncs.com/2021/image-20220804082424264.png)





### 1.6 å¤§æ•°æ®æŠ€æœ¯ç”Ÿæ€ä½“ç³» ğŸˆ

![](https://notes2021.oss-cn-beijing.aliyuncs.com/2021/image-20220804082525280.png)



å›¾ä¸­æ¶‰åŠçš„æŠ€æœ¯åè¯è§£é‡Šå¦‚ä¸‹ï¼šğŸˆ

1ï¼‰Sqoopï¼šSqooæ˜¯ä¸€æ¬¾å¼€æºçš„å·¥å…·ï¼Œä¸»è¦ç”¨äºåœ¨**Hadoopã€Hive ä¸ä¼ ç»Ÿçš„æ•°æ®åº“ (MySQL) é—´è¿›è¡Œæ•°æ®çš„ä¼ é€’**ï¼Œå¯ä»¥å°†ä¸€ä¸ªå…³ç³»å‹æ•°æ®åº“ï¼ˆä¾‹å¦‚ : MySQL, Oracle ç­‰ï¼‰ä¸­çš„æ•°æ®å¯¼è¿›åˆ° Hadoop çš„ `HDFS`ä¸­ï¼Œä¹Ÿå¯ä»¥å°†HDFSçš„æ•°æ®å¯¼è¿›åˆ°å…³ç³»å‹æ•°æ®åº“ä¸­ã€‚

2ï¼‰Flume.ï¼šFlllm æ˜¯ä¸€ä¸ªé«˜å¯ç”¨çš„ã€é«˜å¯é çš„ï¼Œåˆ†å¸ƒå¼çš„**æµ·é‡æ—¥å¿—é‡‡é›†ã€èšåˆå’Œä¼ è¾“çš„ç³»ç»Ÿ**ï¼ŒFlume æ”¯æŒåœ¨æ—¥å¿—ç³»ç»Ÿä¸­å®šåˆ¶å„ç±»æ•°æ®å‘é€æ–¹ï¼Œç”¨äºæ”¶é›†æ•°æ®ï¼›

3ï¼‰Kafkaï¼šKafkaæ˜¯ä¸€ç§**é«˜ååé‡çš„åˆ†å¸ƒå¼å‘å¸ƒè®¢é˜…æ¶ˆæ¯ç³»ç»Ÿ**ï¼›

4ï¼‰Sparkï¼šSpark æ˜¯**å½“å‰æœ€æµè¡Œçš„å¼€æºå¤§æ•°æ®å†…å­˜è®¡ç®—æ¡†æ¶**ã€‚å¯ä»¥åŸºäºHadoopä¸Šå­˜å‚¨çš„å¤§æ•°æ®è¿›è¡Œè®¡ç®—ã€‚

5ï¼‰Flinkï¼šæ˜¯**å½“å‰æœ€æµè¡Œçš„å¼€æºå¤§æ•°æ®å†…å­˜è®¡ç®—æ¡†æ¶**ã€‚ç”¨äº**å®æ—¶è®¡ç®—**çš„åœºæ™¯æ¯”è¾ƒå¤šã€‚

6ï¼‰Oozieï¼šOozieæ˜¯ä¸€ä¸ªç®¡ç† `Hadoop` ä½œä¸š (job) çš„**å·¥ä½œæµç¨‹è°ƒåº¦ç®¡ç†ç³»ç»Ÿ**ã€‚

7ï¼‰Hbase: HBaseæ˜¯ä¸€ä¸ª**åˆ†å¸ƒå¼çš„ã€é¢å‘åˆ—çš„å¼€æºæ•°æ®åº“**ã€‚HBaseä¸åŒäºä¸€èˆ¬çš„å…³ç³»æ•°æ®åº“ï¼Œå®ƒæ˜¯ä¸€ä¸ª**é€‚åˆäºéç»“æ„åŒ–æ•°æ®å­˜å‚¨**çš„æ•°æ®åº“ã€‚

8ï¼‰Hiveï¼šHive æ˜¯åŸºäº Hadoop çš„ä¸€ä¸ªæ•°æ®ä»“åº“å·¥å…·ï¼Œ**å¯ä»¥å°†ç»“æ„åŒ–çš„æ•°æ®æ–‡ä»¶æ˜ å°„ä¸ºä¸€å¼ æ•°æ®åº“è¡¨ï¼Œå¹¶æä¾›ç®€å•çš„SQLæŸ¥è¯¢åŠŸèƒ½**ï¼Œ**å¯ä»¥å°†SQLè¯­å¥è½¬æ¢ä¸º MapReduce ä»»åŠ¡è¿›è¡Œè¿è¡Œ**ã€‚å…¶ä¼˜ç‚¹æ˜¯å­¦ä¹ æˆæœ¬ä½ï¼Œå¯ä»¥é€šè¿‡ç±»SQLè¯­å¥å¿«é€Ÿå®ç°ç®€å•çš„ MapReduce ç»Ÿè®¡ï¼Œä¸å¿…å¼€å‘ä¸“é—¨çš„ MapReduce åº”ç”¨ï¼Œååˆ†**é€‚åˆæ•°æ®ä»“åº“çš„ç»Ÿè®¡åˆ†æ**ã€‚

9ï¼‰ZooKeeperï¼šå®ƒæ˜¯ä¸€ä¸ª**é’ˆå¯¹å¤§å‹åˆ†å¸ƒå¼ç³»ç»Ÿçš„å¯é åè°ƒç³»ç»Ÿ**ï¼Œæä¾›çš„åŠŸèƒ½åŒ…æ‹¬ï¼šé…ç½®ç»´æŠ¤ã€åå­—æœåŠ¡ã€åˆ†å¸ƒå¼åŒæ­¥ã€ç»„æœåŠ¡ç­‰ã€‚

### 1.7 æ¨èç³»ç»Ÿæ¡†æ¶å›¾

![](https://notes2021.oss-cn-beijing.aliyuncs.com/2021/image-20220804083336125.png)





## 2 è¿è¡Œç¯å¢ƒæ­å»ºï¼ˆå¼€å‘é‡ç‚¹ï¼‰

- Linuxï¼š`CentOS-7.5-x86-1804`
- Jdk 8
- Hadoop 3.1.3



### 2.1 æ¨¡æ¿è™šæ‹Ÿæœºç¯å¢ƒå‡†å¤‡

- 0ï¼‰å®‰è£…æ¨¡æ¿è™šæ‹Ÿæœºï¼ŒIPåœ°å€192.168.10.100ã€ä¸»æœºåç§° hadoop100ã€å†…å­˜4Gã€ç¡¬ç›˜50G

- 1ï¼‰hadoop100 è™šæ‹Ÿæœºé…ç½®å¦‚ä¸‹

  - yum å®‰è£…
  - å®‰è£… `epel-release`

  ```bash
  [root@hadoop100 ~]# yum install -y epel-release
  ```

  > æ³¨ï¼šExtra Packages for Enterprise Linuxæ˜¯ä¸ºâ€œçº¢å¸½ç³»â€çš„æ“ä½œç³»ç»Ÿæä¾›é¢å¤–çš„è½¯ä»¶åŒ…ï¼Œé€‚ç”¨äºRHELã€CentOSå’ŒScientific Linuxã€‚ç›¸å½“äºæ˜¯ä¸€ä¸ªè½¯ä»¶ä»“åº“ã€‚

  - å®‰è£… `net-tool`ï¼ˆå·¥å…·åŒ…é›†åˆï¼ŒåŒ…å« ifconfig ç­‰å‘½ä»¤ï¼‰ã€`vim` ç¼–è¾‘å™¨

  ```bash
  [root@hadoop100 ~]# yum install -y net-tools
  
  [root@hadoop100 ~]# yum install -y vim
  ```

  

- 2ï¼‰å…³é—­é˜²ç«å¢™ï¼Œå…³é—­é˜²ç«å¢™å¼€æœºè‡ªå¯

```bash
[root@hadoop100 ~]# systemctl stop firewalld
[root@hadoop100 ~]# systemctl disable firewalld.service
```

> æ³¨æ„ï¼šåœ¨ä¼ä¸šå¼€å‘æ—¶ï¼Œé€šå¸¸å•ä¸ªæœåŠ¡å™¨çš„é˜²ç«å¢™æ—¶å…³é—­çš„ã€‚å…¬å¸æ•´ä½“å¯¹å¤–ä¼šè®¾ç½®éå¸¸å®‰å…¨çš„é˜²ç«å¢™ã€‚

- 3ï¼‰åˆ›å»º atguigu ç”¨æˆ·ï¼Œå¹¶ä¿®æ”¹ atguigu ç”¨æˆ·çš„å¯†ç 

```bash
[root@hadoop100 ~]# useradd atguigu
[root@hadoop100 ~]# passwd atguigu
```

- 4ï¼‰é…ç½®atguiguç”¨æˆ·å…·æœ‰ `root` æƒé™ï¼Œæ–¹ä¾¿åæœŸåŠ  sudo æ‰§è¡Œ root æƒé™çš„å‘½ä»¤

```bash
[root@hadoop100 ~]# vim /etc/sudoers
```

åœ¨ `%wheel` è¿™è¡Œä¸‹é¢æ·»åŠ ä¸€è¡Œï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```bash
## Allow root to run any commands anywhere
root    ALL=(ALL)     ALL

## Allows people in group wheel to run all commands
%wheel  ALL=(ALL)       ALL
atguigu   ALL=(ALL)     NOPASSWD:ALL
```



- 5ï¼‰åœ¨ `/opt` ç›®å½•ä¸‹åˆ›å»ºæ–‡ä»¶å¤¹ï¼Œå¹¶ä¿®æ”¹æ‰€å±ä¸»å’Œæ‰€å±ç»„

  - ï¼ˆ1ï¼‰åœ¨ `/opt` ç›®å½•ä¸‹åˆ›å»º moduleã€software æ–‡ä»¶å¤¹

  ```bash
  [root@hadoop100 ~]# mkdir /opt/module
  [root@hadoop100 ~]# mkdir /opt/software
  ```

  - ï¼ˆ2ï¼‰ä¿®æ”¹ moduleã€software æ–‡ä»¶å¤¹çš„æ‰€æœ‰è€…å’Œæ‰€å±ç»„å‡ä¸º atguigu ç”¨æˆ·

  ```bash
  [root@hadoop100 ~]# chown atguigu:atguigu /opt/module 
  [root@hadoop100 ~]# chown atguigu:atguigu /opt/software
  ```

  - ï¼ˆ3ï¼‰æŸ¥çœ‹ moduleã€software æ–‡ä»¶å¤¹çš„æ‰€æœ‰è€…å’Œæ‰€å±ç»„

  ```bash
  [root@hadoop100 ~]# cd /opt/
  [root@hadoop100 opt]# ll
  æ€»ç”¨é‡ 12
  drwxr-xr-x. 2 atguigu atguigu 4096 5æœˆ  28 17:18 module
  drwxr-xr-x. 2 root    root    4096 9æœˆ   7 2017 rh
  drwxr-xr-x. 2 atguigu atguigu 4096 5æœˆ  28 17:18 software
  ```

  

- 6ï¼‰å¸è½½è™šæ‹Ÿæœºè‡ªå¸¦çš„ JDK

  ```bash
  # æŸ¥çœ‹
  [root@hadoop100 ~]# rpm -qa | grep -i java
  [root@hadoop100 ~]# rpm -qa | grep -i java | xargs -n1 rpm -e --nodeps
  ```

  - rpm -qaï¼šæŸ¥è¯¢æ‰€å®‰è£…çš„æ‰€æœ‰ rpm è½¯ä»¶åŒ…
  - grep -iï¼šå¿½ç•¥å¤§å°å†™
  - xargs -n1ï¼šè¡¨ç¤ºæ¯æ¬¡åªä¼ é€’ä¸€ä¸ªå‚æ•°
  - rpm -e â€“nodepsï¼šå¼ºåˆ¶å¸è½½è½¯ä»¶

- 7ï¼‰é‡å¯è™šæ‹Ÿæœº

  ```bash
  [root@hadoop100 ~]# reboot
  ```

### 2.2 å…‹éš†è™šæ‹Ÿæœº

- 1ï¼‰åˆ©ç”¨æ¨¡æ¿æœº `hadoop100`ï¼Œå…‹éš†ä¸‰å°è™šæ‹Ÿæœºï¼šhadoop102ã€hadoop103ã€hadoop104

> æ³¨æ„ï¼šå…‹éš†æ—¶ï¼Œè¦å…ˆå…³é—­hadoop100ã€‚

- 2ï¼‰ä¿®æ”¹å…‹éš†æœºIPï¼Œä»¥ä¸‹ä»¥hadoop102ä¸¾ä¾‹è¯´æ˜

  - ï¼ˆ1ï¼‰ä¿®æ”¹å…‹éš†è™šæ‹Ÿæœºçš„é™æ€IP

  ```bash
  [root@hadoop100 ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens33vim /etc/sysconfig/network-scripts/ifcfg-ens33
  ä¿®æ”¹ä¸º
  DEVICE=ens33
  TYPE=Ethernet
  ONBOOT=yes
  BOOTPROTO=static
  NAME="ens33"
  IPADDR=192.168.10.102
  PREFIX=24
  GATEWAY=192.168.10.2
  DNS1=192.168.10.2
  ```

  - ï¼ˆ2ï¼‰æŸ¥çœ‹ `Linux` è™šæ‹Ÿæœºçš„è™šæ‹Ÿç½‘ç»œç¼–è¾‘å™¨ï¼Œç¼–è¾‘->è™šæ‹Ÿç½‘ç»œç¼–è¾‘å™¨->VMnet8
  - ï¼ˆ3ï¼‰æŸ¥çœ‹Windowsç³»ç»Ÿé€‚é…å™¨VMware Network Adapter VMnet8çš„IPåœ°å€
  - ï¼ˆ4ï¼‰ä¿è¯ Linux ç³»ç»Ÿ ifcfg-ens33 æ–‡ä»¶ä¸­IPåœ°å€ã€è™šæ‹Ÿç½‘ç»œç¼–è¾‘å™¨åœ°å€å’ŒWindowsç³»ç»Ÿ VM8 ç½‘ç»œ IP åœ°å€ç›¸åŒã€‚

- 3ï¼‰ä¿®æ”¹å…‹éš†æœºä¸»æœºåï¼Œä»¥ä¸‹ä»¥hadoop102ä¸¾ä¾‹è¯´æ˜

  - ï¼ˆ1ï¼‰ä¿®æ”¹ä¸»æœºåç§°

  ```bash
  [root@hadoop100 ~]# vim /etc/hostname
  hadoop102
  ```

  - ï¼ˆ2ï¼‰é…ç½®Linuxå…‹éš†æœºä¸»æœºåç§°æ˜ å°„hostsæ–‡ä»¶ï¼Œæ‰“å¼€ `/etc/hosts`

  ```bash
  [root@hadoop100 ~]# vim /etc/hosts
  ```

  æ·»åŠ å¦‚ä¸‹å†…å®¹

  ```bash
  192.168.10.100 hadoop100
  192.168.10.101 hadoop101
  192.168.10.102 hadoop102
  192.168.10.103 hadoop103
  192.168.10.104 hadoop104
  192.168.10.105 hadoop105
  192.168.10.106 hadoop106
  192.168.10.107 hadoop107
  192.168.10.108 hadoop108
  ```

- 4ï¼‰é‡å¯å…‹éš†æœºhadoop102

```bash
[root@hadoop100 ~]# reboot
```

- 5ï¼‰ä¿®æ”¹ windows çš„ä¸»æœºæ˜ å°„æ–‡ä»¶ï¼ˆhostsæ–‡ä»¶ï¼‰

  - ï¼ˆ1ï¼‰å¦‚æœæ“ä½œç³»ç»Ÿæ˜¯ window7ï¼Œå¯ä»¥ç›´æ¥ä¿®æ”¹

  - ï¼ˆ2ï¼‰å¦‚æœæ“ä½œç³»ç»Ÿæ˜¯ window10ï¼Œå…ˆæ‹·è´å‡ºæ¥ï¼Œä¿®æ”¹ä¿å­˜ä»¥åï¼Œå†è¦†ç›–å³å¯

    - ï¼ˆaï¼‰è¿›å…¥ `C:\Windows\System32\drivers\etc` è·¯å¾„
    - ï¼ˆbï¼‰æ‹·è´ hosts æ–‡ä»¶åˆ°æ¡Œé¢
    - ï¼ˆcï¼‰æ‰“å¼€æ¡Œé¢ hosts æ–‡ä»¶å¹¶æ·»åŠ å¦‚ä¸‹å†…å®¹

    ```bash
    192.168.10.100 hadoop100
    192.168.10.101 hadoop101
    192.168.10.102 hadoop102
    192.168.10.103 hadoop103
    192.168.10.104 hadoop104
    192.168.10.105 hadoop105
    192.168.10.106 hadoop106
    192.168.10.107 hadoop107
    192.168.10.108 hadoop108
    ```

    - ï¼ˆdï¼‰å°†æ¡Œé¢ hosts æ–‡ä»¶è¦†ç›– `C:\Windows\System32\drivers\etc` è·¯å¾„ `hosts `æ–‡ä»¶

### 2.3 åœ¨ hadoop102 å®‰è£… JDK

- 1ï¼‰å¸è½½ç°æœ‰ JDK

> æ³¨æ„ï¼šå®‰è£… JDK å‰ï¼Œä¸€å®šç¡®ä¿æå‰åˆ é™¤äº†è™šæ‹Ÿæœºè‡ªå¸¦çš„ JDKã€‚

- 2ï¼‰ç”¨ XShell ç­‰ä¼ è¾“å·¥å…·å°† JDK å¯¼å…¥åˆ° `/opt/software/` ç›®å½•ä¸‹ï¼Œå¹¶æŸ¥çœ‹

  ```bash
  [atguigu@hadoop102 ~]$ ls /opt/software/
  ```

- 3ï¼‰è§£å‹ JDK åˆ° `/opt/module` ç›®å½•ä¸‹

  ```bash
  [atguigu@hadoop102 software]$ tar -zxvf jdk-8u212-linux-x64.tar.gz -C /opt/module/
  ```

- 4ï¼‰é…ç½® JDK ç¯å¢ƒå˜é‡

```bash
ï¼ˆ1ï¼‰æ–°å»º/etc/profile.d/my_env.shæ–‡ä»¶
[atguigu@hadoop102 ~]$ sudo vim /etc/profile.d/my_env.sh
æ·»åŠ å¦‚ä¸‹å†…å®¹

#JAVA_HOME
export JAVA_HOME=/opt/module/jdk1.8.0_212
export PATH=$PATH:$JAVA_HOME/bin

ï¼ˆ2ï¼‰ä¿å­˜åé€€å‡º
:wq

ï¼ˆ3ï¼‰sourceä¸€ä¸‹/etc/profileæ–‡ä»¶ï¼Œè®©æ–°çš„ç¯å¢ƒå˜é‡ PATH ç”Ÿæ•ˆ
[atguigu@hadoop102 ~]$ source /etc/profile
```

- 6ï¼‰æµ‹è¯• JDK æ˜¯å¦å®‰è£…æˆåŠŸ

```bash
[atguigu@hadoop102 ~]$ java -version

java version "1.8.0_212"
```



### 2.4 åœ¨ hadoop102 å®‰è£…Hadoop

Hadoopä¸‹è½½åœ°å€ï¼š[https://archive.apache.org/dist/hadoop/common/hadoop-3.1.3/](https://archive.apache.org/dist/hadoop/common/hadoop-2.7.2/)

- 1ï¼‰ç”¨ XShell ç­‰æ–‡ä»¶ä¼ è¾“å·¥å…·å°† hadoop-3.1.3.tar.gz å¯¼å…¥åˆ°  `/opt/software/` ç›®å½•ä¸‹

- 2ï¼‰è¿›å…¥åˆ° Hadoop å®‰è£…åŒ…è·¯å¾„ä¸‹

  ```bash
  [atguigu@hadoop102 ~]$ cd /opt/software/
  ```

- 3ï¼‰è§£å‹å®‰è£…æ–‡ä»¶åˆ° /opt/module ä¸‹é¢ï¼Œå¹¶æŸ¥çœ‹

  ```bash
  [atguigu@hadoop102 software]$ tar -zxvf hadoop-3.1.3.tar.gz -C /opt/module/
  
  [atguigu@hadoop102 software]$ ls /opt/module/
  hadoop-3.1.3
  ```

- 4ï¼‰å°†Hadoopæ·»åŠ åˆ°ç¯å¢ƒå˜é‡

  - ï¼ˆ1ï¼‰è·å–Hadoopå®‰è£…è·¯å¾„

  ```bash
  [atguigu@hadoop102 hadoop-3.1.3]$ pwd
  /opt/module/hadoop-3.1.3
  ```

  - ï¼ˆ2ï¼‰æ‰“å¼€ /etc/profile.d/my_env.sh æ–‡ä»¶

  ```bash
  [atguigu@hadoop102 hadoop-3.1.3]$ sudo vim /etc/profile.d/my_env.sh
  
  åœ¨my_env.shæ–‡ä»¶æœ«å°¾æ·»åŠ å¦‚ä¸‹å†…å®¹ï¼šï¼ˆshift+gï¼‰
  
  #HADOOP_HOME
  export HADOOP_HOME=/opt/module/hadoop-3.1.3
  export PATH=$PATH:$HADOOP_HOME/bin
  export PATH=$PATH:$HADOOP_HOME/sbin
  
  ä¿å­˜å¹¶é€€å‡ºï¼š :wq
  ```

  - ï¼ˆ3ï¼‰è®©ä¿®æ”¹åçš„æ–‡ä»¶ç”Ÿæ•ˆ

  ```bash
  [atguigu@hadoop102 hadoop-3.1.3]$ source /etc/profile
  ```

- 5ï¼‰æµ‹è¯•æ˜¯å¦å®‰è£…æˆåŠŸ

```bash
[atguigu@hadoop102 hadoop-3.1.3]$ hadoop version
Hadoop 3.1.3
```

- 6ï¼‰é‡å¯ï¼ˆå¦‚æœHadoopå‘½ä»¤ä¸èƒ½ç”¨å†é‡å¯è™šæ‹Ÿæœºï¼‰

```bash
[atguigu@hadoop102 hadoop-3.1.3]$ sudo reboot
```



### 2.5 Hadoop ç›®å½•ç»“æ„

```bash
[atguigu@hadoop102 hadoop-3.1.3]$ ll
æ€»ç”¨é‡ 52
drwxr-xr-x. 2 atguigu atguigu  4096 5æœˆ  22 2017 bin
drwxr-xr-x. 3 atguigu atguigu  4096 5æœˆ  22 2017 etc
drwxr-xr-x. 2 atguigu atguigu  4096 5æœˆ  22 2017 include
drwxr-xr-x. 3 atguigu atguigu  4096 5æœˆ  22 2017 lib
drwxr-xr-x. 2 atguigu atguigu  4096 5æœˆ  22 2017 libexec
-rw-r--r--. 1 atguigu atguigu 15429 5æœˆ  22 2017 LICENSE.txt
-rw-r--r--. 1 atguigu atguigu   101 5æœˆ  22 2017 NOTICE.txt
-rw-r--r--. 1 atguigu atguigu  1366 5æœˆ  22 2017 README.txt
drwxr-xr-x. 2 atguigu atguigu  4096 5æœˆ  22 2017 sbin
drwxr-xr-x. 4 atguigu atguigu  4096 5æœˆ  22 2017 share
```

- é‡è¦ç›®å½•
  - ï¼ˆ1ï¼‰**`bin`** ç›®å½•ï¼šå­˜æ”¾å¯¹ Hadoop ç›¸å…³æœåŠ¡ï¼ˆ`hdfs`ï¼Œ`yarn`ï¼Œ`mapred`ï¼‰è¿›è¡Œæ“ä½œçš„è„šæœ¬
  - ï¼ˆ2ï¼‰**`etc` ** ç›®å½•ï¼šHadoopçš„é…ç½®æ–‡ä»¶ç›®å½•ï¼Œå­˜æ”¾Hadoopçš„é…ç½®æ–‡ä»¶
  - ï¼ˆ3ï¼‰lib ç›®å½•ï¼šå­˜æ”¾Hadoopçš„æœ¬åœ°åº“ï¼ˆå¯¹æ•°æ®è¿›è¡Œå‹ç¼©è§£å‹ç¼©åŠŸèƒ½ï¼‰
  - ï¼ˆ4ï¼‰**`sbin `** ç›®å½•ï¼šå­˜æ”¾å¯åŠ¨æˆ–åœæ­¢Hadoopç›¸å…³æœåŠ¡çš„è„šæœ¬
  - ï¼ˆ5ï¼‰share ç›®å½•ï¼šå­˜æ”¾Hadoopçš„ä¾èµ– jar åŒ…ã€æ–‡æ¡£ã€å’Œå®˜æ–¹æ¡ˆä¾‹



## 3 è¿è¡Œæ¨¡å¼

- Hadoopå®˜æ–¹ç½‘ç«™ï¼šhttp://hadoop.apache.org/

- Hadoopè¿è¡Œæ¨¡å¼åŒ…æ‹¬ï¼šæœ¬åœ°æ¨¡å¼ã€ä¼ªåˆ†å¸ƒå¼æ¨¡å¼ä»¥åŠå®Œå…¨åˆ†å¸ƒå¼æ¨¡å¼ã€‚
  - æœ¬åœ°æ¨¡å¼ï¼šå•æœºè¿è¡Œï¼Œåªæ˜¯ç”¨æ¥æ¼”ç¤ºä¸€ä¸‹å®˜æ–¹æ¡ˆä¾‹ã€‚ç”Ÿäº§ç¯å¢ƒä¸ç”¨ã€‚
  - ä¼ªåˆ†å¸ƒå¼æ¨¡å¼ï¼šä¹Ÿæ˜¯å•æœºè¿è¡Œï¼Œä½†æ˜¯å…·å¤‡Hadoopé›†ç¾¤çš„æ‰€æœ‰åŠŸèƒ½ï¼Œä¸€å°æœåŠ¡å™¨æ¨¡æ‹Ÿä¸€ä¸ªåˆ†å¸ƒå¼çš„ç¯å¢ƒã€‚ä¸ªåˆ«ç¼ºé’±çš„å…¬å¸ç”¨æ¥æµ‹è¯•ï¼Œç”Ÿäº§ç¯å¢ƒä¸ç”¨ã€‚
  - å®Œå…¨åˆ†å¸ƒå¼æ¨¡å¼ï¼šå¤šå°æœåŠ¡å™¨ç»„æˆåˆ†å¸ƒå¼ç¯å¢ƒã€‚**ç”Ÿäº§ç¯å¢ƒä½¿ç”¨ã€‚**

![](https://notes2021.oss-cn-beijing.aliyuncs.com/2021/image-20220809194929194.png)



### 3.1 æœ¬åœ°è¿è¡Œæ¨¡å¼ï¼ˆå®˜æ–¹WordCountï¼‰

- åœ¨ `hadoop-3.1.3` æ–‡ä»¶ä¸‹é¢åˆ›å»ºä¸€ä¸ª `wcinput` æ–‡ä»¶å¤¹

```bash
[atguigu@hadoop102 hadoop-3.1.3]$ mkdir wcinput
```

- åœ¨ wcinput æ–‡ä»¶ä¸‹åˆ›å»ºä¸€ä¸ª `word.txt` æ–‡ä»¶

```bash
[atguigu@hadoop102 hadoop-3.1.3]$ cd wcinput
```

- ç¼–è¾‘`word.txt` æ–‡ä»¶

```bash
[atguigu@hadoop102 wcinput]$ vim word.txt

# åœ¨æ–‡ä»¶ä¸­è¾“å…¥å¦‚ä¸‹å†…å®¹
hadoop yarn
hadoop mapreduce
atguigu
atguigu
```

- å›åˆ° Hadoopç›®å½• ` /opt/module/hadoop-3.1.3`ï¼Œæ‰§è¡Œç¨‹åº

```bash
[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount wcinput wcoutput
```

- æŸ¥çœ‹ç»“æœ

```bash
[atguigu@hadoop102 hadoop-3.1.3]$ cat wcoutput/part-r-00000

# ç»“æœå¦‚ä¸‹
atguigu 2
hadoop  2
mapreduce       1
yarn    1
```



### 3.2 å®Œå…¨åˆ†å¸ƒå¼è¿è¡Œæ¨¡å¼ğŸˆï¼ˆå¼€å‘é‡ç‚¹ï¼‰

- åˆ†æï¼š

  - 1ï¼‰å‡†å¤‡3å°å®¢æˆ·æœºï¼ˆå…³é—­é˜²ç«å¢™ã€é™æ€IPã€ä¸»æœºåç§°ï¼‰
  - 2ï¼‰å®‰è£…JDKã€é…ç½®ç¯å¢ƒå˜é‡
  - 3ï¼‰å®‰è£…Hadoopã€é…ç½®ç¯å¢ƒå˜é‡
  - 4ï¼‰é…ç½®é›†ç¾¤
  - 5ï¼‰å•ç‚¹å¯åŠ¨
  - 6ï¼‰é…ç½®ssh
  - 7ï¼‰ç¾¤èµ·å¹¶æµ‹è¯•é›†ç¾¤



#### 3.2.0 æ€»ç»“ä¸€ä¸‹

|      | hadoop102         | hadoop103                   | hadoop104                  |
| ---- | ----------------- | --------------------------- | -------------------------- |
| HDFS | NameNode DataNode | DataNode                    | SecondaryNameNode DataNode |
| YARN | NodeManager       | ResourceManager NodeManager | NodeManager                |

- è™šæ‹Ÿæœºå‡†å¤‡ï¼ˆè¯¦è§ 2.1 ~ 2.4 ï¼‰
  - hadoop102 å®‰è£… JDKã€Hadoop
  - å…³é—­é”å±

- é›†ç¾¤åˆ†å‘è„šæœ¬
  - `scp` å‘½ä»¤æ‹·è´ 
  - `rsync` è¿œç¨‹åŒæ­¥ï¼ˆåŒæ­¥ hadoop102 ä¸Šçš„ JDKã€Hadoopã€ç¯å¢ƒå˜é‡é…ç½®åˆ° hadoop103ã€hadoop104 ï¼‰
  - å®ç° `xsync` é›†ç¾¤åˆ†å‘è„šæœ¬

- ssh æ— å¯†ç™»é™†é…ç½®ï¼ˆä¸‰å°æœºå™¨åŒæ ·é…ç½®ï¼‰

```bash
$ ssh-keygen -t rsa
$ ssh-copy-id hadoop102
$ ssh-copy-id hadoop103
$ ssh-copy-id hadoop104
```

- ä¿®æ”¹ hadoop102 é…ç½®æ–‡ä»¶å¹¶åˆ†å‘
  - æ ¸å¿ƒé…ç½®æ–‡ä»¶ï¼š`core-site.xml`
  - HDFS é…ç½®æ–‡ä»¶ï¼šé…ç½® `hdfs-site.xml`
  - YARN é…ç½®æ–‡ä»¶ï¼šé…ç½® `yarn-site.xml`
  - MapReduce é…ç½®æ–‡ä»¶ï¼šé…ç½® `mapred-site.xml`

- å¯åŠ¨é›†ç¾¤

  - hadoop102 é…ç½® `workers` å¹¶åˆ†å‘
  - hadoop102 æ ¼å¼åŒ– `NameNode`
  - hadoop102 å¯åŠ¨ `hdfs`

  ```bash
  # å¯åŠ¨ hdfs
  [atguigu@hadoop102 hadoop-3.1.3]$ sbin/start-dfs.sh
  ```

  - hadoop103 å¯åŠ¨ `yarn`

  ```bash
  [atguigu@hadoop103 hadoop-3.1.3]$ sbin/start-yarn.sh
  ```

  - web é¡µé¢æŸ¥çœ‹ï¼šhttp://hadoop102:9870ã€ http://hadoop103:8088

- é…ç½®å†å²æœåŠ¡å™¨
- é…ç½®æ—¥å¿— 





#### 3.2.1 è™šæ‹Ÿæœºå‡†å¤‡

- è¯¦è§ 2.1 ~ 2.4 
- è™šæ‹Ÿæœºå…³é—­é”å±ï¼ˆä¸€ç›´è¾“å¯†ç è¿›ç³»ç»Ÿå¾ˆéº»çƒ¦ï¼ï¼‰

![](https://notes2021.oss-cn-beijing.aliyuncs.com/2021/image-20220824220241066.png)



#### 3.2.2 ç¼–å†™é›†ç¾¤åˆ†å‘è„šæœ¬xsync â­ï¸

> - æ‹·è´
> - åŒæ­¥
> - è„šæœ¬



![](https://notes2021.oss-cn-beijing.aliyuncs.com/2021/image-20220824223022082.png)



- scpï¼ˆsecure copyï¼‰å®‰å…¨æ‹·è´

  - å®šä¹‰ï¼šscpå¯ä»¥å®ç°æœåŠ¡å™¨ä¸æœåŠ¡å™¨ä¹‹é—´çš„æ•°æ®æ‹·è´ã€‚ï¼ˆfrom server1 to server2ï¼‰

  - è¯­æ³•

    ```bash
    scp  -r     $pdir/$fname       $user@$host:$pdir/$fname
    
    å‘½ä»¤  é€’å½’   è¦æ‹·è´çš„æ–‡ä»¶è·¯å¾„/åç§°  ç›®çš„åœ°ç”¨æˆ·@ä¸»æœº:ç›®çš„åœ°è·¯å¾„/åç§°
    ```

    

  - æ¡ˆä¾‹å®æ“

  ```bash
  # å‰æï¼šåœ¨hadoop102ã€hadoop103ã€hadoop104éƒ½å·²ç»åˆ›å»ºå¥½/opt/moduleã€/opt/softwareä¸¤ä¸ªç›®å½•ï¼Œ
  # å¹¶ä¸”å·²ç»æŠŠè¿™ä¸¤ä¸ªç›®å½•ä¿®æ”¹ä¸ºatguigu:atguigu
  [atguigu@hadoop102 ~]$ sudo chown atguigu:atguigu -R /opt/module
  
  # å°†hadoop102ä¸­/opt/module/jdk1.8.0_212ç›®å½•æ‹·è´åˆ°hadoop103ä¸Š
  [atguigu@hadoop102 ~]$ scp -r /opt/module/jdk1.8.0_212  atguigu@hadoop103:/opt/module
  
  # å°†hadoop102ä¸­/opt/module/hadoop-3.1.3ç›®å½•æ‹·è´åˆ°hadoop103ä¸Š
  [atguigu@hadoop103 ~]$ scp -r atguigu@hadoop102:/opt/module/hadoop-3.1.3 /opt/module/
  
  # å°†hadoop102ä¸­/opt/moduleç›®å½•ä¸‹æ‰€æœ‰ç›®å½•æ‹·è´åˆ°hadoop104ä¸Š
  [atguigu@hadoop103 opt]$ scp -r atguigu@hadoop102:/opt/module/* atguigu@hadoop104:/opt/module
  ```

  

- rsync è¿œç¨‹åŒæ­¥å·¥å…·
  - rsync ä¸»è¦ç”¨äºå¤‡ä»½å’Œé•œåƒã€‚å…·æœ‰é€Ÿåº¦å¿«ã€é¿å…å¤åˆ¶ç›¸åŒå†…å®¹å’Œæ”¯æŒç¬¦å·é“¾æ¥çš„ä¼˜ç‚¹ã€‚
  
  - rsync å’Œ scp åŒºåˆ«ï¼šç”¨ rsync åšæ–‡ä»¶çš„å¤åˆ¶è¦æ¯” scp çš„é€Ÿåº¦å¿«ï¼Œrsync **åªå¯¹å·®å¼‚æ–‡ä»¶åšæ›´æ–°**ï¼Œscp æ˜¯**æŠŠæ‰€æœ‰æ–‡ä»¶éƒ½å¤åˆ¶è¿‡å»**ã€‚
  
  - è¯­æ³•
  
    ```bash
    rsync   -av    $pdir/$fname       $user@$host:$pdir/$fname
    
    å‘½ä»¤  é€‰é¡¹å‚æ•°  è¦æ‹·è´çš„æ–‡ä»¶è·¯å¾„/åç§°  ç›®çš„åœ°ç”¨æˆ·@ä¸»æœº:ç›®çš„åœ°è·¯å¾„/åç§°
    ```
  
     é€‰é¡¹å‚æ•°è¯´æ˜
  
    | é€‰é¡¹ | åŠŸèƒ½         |
    | ---- | ------------ |
    | -a   | å½’æ¡£æ‹·è´     |
    | -v   | æ˜¾ç¤ºå¤åˆ¶è¿‡ç¨‹ |
  
  - æ¡ˆä¾‹å®æ“
  
  ```bash
  # åˆ é™¤hadoop103ä¸­/opt/module/hadoop-3.1.3/wcinput
  [atguigu@hadoop103 hadoop-3.1.3]$ rm -rf wcinput/
  
  # åŒæ­¥hadoop102ä¸­çš„/opt/module/hadoop-3.1.3åˆ°hadoop103
  [atguigu@hadoop102 module]$ rsync -av hadoop-3.1.3/ atguigu@hadoop103:/opt/module/hadoop-3.1.3/
  ```
  
  
  
- `xsync` é›†ç¾¤åˆ†å‘è„šæœ¬

  - éœ€æ±‚ï¼šå¾ªç¯å¤åˆ¶æ–‡ä»¶åˆ°æ‰€æœ‰èŠ‚ç‚¹çš„ç›¸åŒç›®å½•ä¸‹

  - åˆ†æ

    - `rsync` å‘½ä»¤åŸå§‹æ‹·è´

    ```bash
    rsync  -av     /opt/module  		 atguigu@hadoop103:/opt/
    ```

    - æœŸæœ›è„šæœ¬ï¼šxsync è¦åŒæ­¥çš„æ–‡ä»¶åç§°
    - æœŸæœ›è„šæœ¬åœ¨ä»»ä½•è·¯å¾„éƒ½èƒ½ä½¿ç”¨ï¼ˆè„šæœ¬æ”¾åœ¨å£°æ˜äº†å…¨å±€ç¯å¢ƒå˜é‡çš„è·¯å¾„ï¼‰

    ```bash
    [atguigu@hadoop102 ~]$ echo $PATH
    /usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/atguigu/.local/bin:/home/atguigu/bin:/opt/module/jdk1.8.0_212/bin
    ```

    

  - è„šæœ¬å®ç°

    - åœ¨ /home/atguigu/bin ç›®å½•ä¸‹åˆ›å»º `xsync` æ–‡ä»¶

    ```bash
    [atguigu@hadoop102 opt]$ cd /home/atguigu
    [atguigu@hadoop102 ~]$ mkdir bin
    [atguigu@hadoop102 ~]$ cd bin
    [atguigu@hadoop102 bin]$ vim xsync
    # åœ¨è¯¥æ–‡ä»¶ä¸­ç¼–å†™å¦‚ä¸‹ä»£ç 
    ```
    
    
    
    ```bash
    #!/bin/bash
    
    #1. åˆ¤æ–­å‚æ•°ä¸ªæ•°
    if [ $# -lt 1 ]
    then
        echo Not Enough Arguement!
        exit;
    fi
    
    #2. éå†é›†ç¾¤æ‰€æœ‰æœºå™¨
    for host in hadoop102 hadoop103 hadoop104
    do
        echo ====================  $host  ====================
        #3. éå†æ‰€æœ‰ç›®å½•ï¼ŒæŒ¨ä¸ªå‘é€
    
        for file in $@
        do
            #4. åˆ¤æ–­æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if [ -e $file ]
                then
                    #5. è·å–çˆ¶ç›®å½•
                    pdir=$(cd -P $(dirname $file); pwd)
    
                    #6. è·å–å½“å‰æ–‡ä»¶çš„åç§°
                    fname=$(basename $file)
                    ssh $host "mkdir -p $pdir"
                    rsync -av $pdir/$fname $host:$pdir
                else
                    echo $file does not exists!
            fi
        done
    done
    ```
    
    - ä¿®æ”¹è„šæœ¬ xsync å…·æœ‰æ‰§è¡Œæƒé™

    ```bash
    [atguigu@hadoop102 bin]$ chmod +x xsync
    ```
    
    - æµ‹è¯•è„šæœ¬

    ```bash
    [atguigu@hadoop102 ~]$ xsync /home/atguigu/bin
    ```
    
    - å°†è„šæœ¬å¤åˆ¶åˆ° /bin ä¸­ï¼Œä»¥ä¾¿å…¨å±€è°ƒç”¨

    ```bash
    [atguigu@hadoop102 bin]$ sudo cp xsync /bin/
    ```
    
    - åŒæ­¥ç¯å¢ƒå˜é‡é…ç½®ï¼ˆrootæ‰€æœ‰è€…ï¼‰
    
    ```bash
    [atguigu@hadoop102 ~]$ sudo ./bin/xsync /etc/profile.d/my_env.sh
    
    # è®©ç¯å¢ƒå˜é‡ç”Ÿæ•ˆ
    [atguigu@hadoop103 bin]$ source /etc/profile
    [atguigu@hadoop104 opt]$ source /etc/profile
    ```

#### 3.2.3 SSH æ— å¯†ç™»å½•é…ç½®

- é…ç½® ssh

  - è¯­æ³•ï¼š`ssh` å¦ä¸€å°ç”µè„‘çš„IPåœ°å€
  - ssh è¿æ¥æ—¶å‡ºç° `Host key verification failed` çš„è§£å†³æ–¹æ³•

  ```bash
  [atguigu@hadoop102 ~]$ ssh hadoop103
  # å¦‚æœå‡ºç°å¦‚ä¸‹å†…å®¹
  Are you sure you want to continue connecting (yes/no)? 
  # è¾“å…¥yesï¼Œå¹¶å›è½¦
  # é€€å›åˆ°hadoop102
  [atguigu@hadoop103 ~]$ exit
  ```

  

- æ— å¯†é’¥é…ç½®

  - å…å¯†ç™»å½•åŸç†
  - ç”Ÿæˆå…¬é’¥å’Œç§é’¥

  ```bash
  [atguigu@hadoop102 .ssh]$ pwd
  /home/atguigu/.ssh
  
  [atguigu@hadoop102 .ssh]$ ssh-keygen -t rsa
  # ç„¶åæ•²ï¼ˆä¸‰ä¸ªå›è½¦ï¼‰ï¼Œå°±ä¼šç”Ÿæˆä¸¤ä¸ªæ–‡ä»¶id_rsaï¼ˆç§é’¥ï¼‰ã€id_rsa.pubï¼ˆå…¬é’¥ï¼‰
  # å°†å…¬é’¥æ‹·è´åˆ°è¦å…å¯†ç™»å½•çš„ç›®æ ‡æœºå™¨ä¸Š
  [atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop102
  [atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop103
  [atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop104
  
  # æ³¨æ„ï¼š
  è¿˜éœ€è¦åœ¨hadoop103ä¸Šé‡‡ç”¨atguiguè´¦å·é…ç½®ä¸€ä¸‹æ— å¯†ç™»å½•åˆ°hadoop102ã€hadoop103ã€hadoop104æœåŠ¡å™¨ä¸Šã€‚
  è¿˜éœ€è¦åœ¨hadoop104ä¸Šé‡‡ç”¨atguiguè´¦å·é…ç½®ä¸€ä¸‹æ— å¯†ç™»å½•åˆ°hadoop102ã€hadoop103ã€hadoop104æœåŠ¡å™¨ä¸Šã€‚
  ```




- .ssh æ–‡ä»¶å¤¹ä¸‹ï¼ˆ~/.sshï¼‰çš„æ–‡ä»¶åŠŸèƒ½è§£é‡Š

| æ–‡ä»¶            | åŠŸèƒ½                                    |
| --------------- | --------------------------------------- |
| known_hosts     | è®°å½•sshè®¿é—®è¿‡è®¡ç®—æœºçš„å…¬é’¥ï¼ˆpublic keyï¼‰ |
| id_rsa          | ç”Ÿæˆçš„ç§é’¥                              |
| id_rsa.pub      | ç”Ÿæˆçš„å…¬é’¥                              |
| authorized_keys | å­˜æ”¾æˆæƒè¿‡çš„æ— å¯†ç™»å½•æœåŠ¡å™¨å…¬é’¥          |

#### 3.2.4 é›†ç¾¤é…ç½®

- é›†ç¾¤éƒ¨ç½²è§„åˆ’
  - NameNode å’Œ SecondaryNameNode ä¸è¦å®‰è£…åœ¨åŒä¸€å°æœåŠ¡å™¨
  - ResourceManager ä¹Ÿå¾ˆæ¶ˆè€—å†…å­˜ï¼Œä¸è¦å’Œ NameNodeã€SecondaryNameNode é…ç½®åœ¨åŒä¸€å°æœºå™¨ä¸Šã€‚

|      | hadoop102         | hadoop103                   | hadoop104                  |
| ---- | ----------------- | --------------------------- | -------------------------- |
| HDFS | NameNode DataNode | DataNode                    | SecondaryNameNode DataNode |
| YARN | NodeManager       | ResourceManager NodeManager | NodeManager                |

- é…ç½®æ–‡ä»¶è¯´æ˜

  - Hadoop é…ç½®æ–‡ä»¶åˆ†ä¸¤ç±»ï¼šé»˜è®¤é…ç½®æ–‡ä»¶å’Œè‡ªå®šä¹‰é…ç½®æ–‡ä»¶ï¼Œåªæœ‰ç”¨æˆ·æƒ³ä¿®æ”¹æŸä¸€é»˜è®¤é…ç½®å€¼æ—¶ï¼Œæ‰éœ€è¦ä¿®æ”¹è‡ªå®šä¹‰é…ç½®æ–‡ä»¶ï¼Œæ›´æ”¹ç›¸åº”å±æ€§å€¼ã€‚
  - é»˜è®¤é…ç½®æ–‡ä»¶

  | è¦è·å–çš„é»˜è®¤æ–‡ä»¶     | æ–‡ä»¶å­˜æ”¾åœ¨Hadoopçš„jaråŒ…ä¸­çš„ä½ç½®                           |
  | -------------------- | --------------------------------------------------------- |
  | [core-default.xml]   | hadoop-common-3.1.3.jar/core-default.xml                  |
  | [hdfs-default.xml]   | hadoop-hdfs-3.1.3.jar/hdfs-default.xml                    |
  | [yarn-default.xml]   | hadoop-yarn-common-3.1.3.jar/yarn-default.xml             |
  | [mapred-default.xml] | hadoop-mapreduce-client-core-3.1.3.jar/mapred-default.xml |

  - è‡ªå®šä¹‰é…ç½®æ–‡ä»¶

  core-site.xmlã€hdfs-site.xmlã€yarn-site.xmlã€mapred-site.xml å››ä¸ªé…ç½®æ–‡ä»¶å­˜æ”¾åœ¨$HADOOP_HOME/etc/hadoopè¿™ä¸ªè·¯å¾„ä¸Šï¼Œç”¨æˆ·å¯ä»¥æ ¹æ®é¡¹ç›®éœ€æ±‚é‡æ–°è¿›è¡Œä¿®æ”¹é…ç½®ã€‚

- é…ç½®é›†ç¾¤

  - æ ¸å¿ƒé…ç½®æ–‡ä»¶ï¼šé…ç½® `core-site.xml`

  ```bash
  [atguigu@hadoop102 ~]$ cd $HADOOP_HOME/etc/hadoop
  [atguigu@hadoop102 hadoop]$ vim core-site.xml
  ```

  å†…å®¹å¦‚ä¸‹ï¼š

  ```xml
  <?xml version="1.0" encoding="UTF-8"?>
  <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
  
  <configuration>
      <!-- æŒ‡å®šNameNodeçš„åœ°å€ -->
      <property>
          <name>fs.defaultFS</name>
          <value>hdfs://hadoop102:8020</value>
      </property>
  
      <!-- æŒ‡å®šhadoopæ•°æ®çš„å­˜å‚¨ç›®å½• -->
      <property>
          <name>hadoop.tmp.dir</name>
          <value>/opt/module/hadoop-3.1.3/data</value>
      </property>
  
      <!-- é…ç½®HDFSç½‘é¡µç™»å½•ä½¿ç”¨çš„é™æ€ç”¨æˆ·ä¸ºatguigu -->
      <property>
          <name>hadoop.http.staticuser.user</name>
          <value>atguigu</value>
      </property>
  </configuration>
  ```

  

  - HDFS é…ç½®æ–‡ä»¶ï¼šé…ç½® `hdfs-site.xml`

  ```bash
  [atguigu@hadoop102 hadoop]$ vim hdfs-site.xml
  ```

  å†…å®¹å¦‚ä¸‹ï¼š

  ```xml
  <?xml version="1.0" encoding="UTF-8"?>
  <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
  
  <configuration>
  	<!-- nn webç«¯è®¿é—®åœ°å€-->
  	<property>
          <name>dfs.namenode.http-address</name>
          <value>hadoop102:9870</value>
      </property>
  	<!-- 2nn webç«¯è®¿é—®åœ°å€-->
      <property>
          <name>dfs.namenode.secondary.http-address</name>
          <value>hadoop104:9868</value>
      </property>
  </configuration>
  ```

  

  - YARN é…ç½®æ–‡ä»¶ï¼šé…ç½® `yarn-site.xml`

  ```bash
  [atguigu@hadoop102 hadoop]$ vim yarn-site.xml
  ```

  å†…å®¹å¦‚ä¸‹ï¼š

  ```xml
  <?xml version="1.0" encoding="UTF-8"?>
  <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
  
  <configuration>
      <!-- æŒ‡å®šMRèµ°shuffle -->
      <property>
          <name>yarn.nodemanager.aux-services</name>
          <value>mapreduce_shuffle</value>
      </property>
  
      <!-- æŒ‡å®šResourceManagerçš„åœ°å€-->
      <property>
          <name>yarn.resourcemanager.hostname</name>
          <value>hadoop103</value>
      </property>
  
      <!-- ç¯å¢ƒå˜é‡çš„ç»§æ‰¿ -->
      <property>
          <name>yarn.nodemanager.env-whitelist</name>
          <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
      </property>
  </configuration>
  ```

  - MapReduce é…ç½®æ–‡ä»¶ï¼šé…ç½® `mapred-site.xml`

  ```bash
  [atguigu@hadoop102 hadoop]$ vim mapred-site.xml
  ```

  å†…å®¹å¦‚ä¸‹ï¼š

  ```xml
  <?xml version="1.0" encoding="UTF-8"?>
  <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
  
  <configuration>
  	<!-- æŒ‡å®šMapReduceç¨‹åºè¿è¡Œåœ¨Yarnä¸Š -->
      <property>
          <name>mapreduce.framework.name</name>
          <value>yarn</value>
      </property>
  </configuration>
  ```

  

- åœ¨é›†ç¾¤ä¸Šåˆ†å‘é…ç½®å¥½çš„ Hadoop é…ç½®æ–‡ä»¶

```bash
[atguigu@hadoop102 hadoop]$ xsync /opt/module/hadoop-3.1.3/etc/hadoop/
```



- å»103å’Œ104ä¸ŠæŸ¥çœ‹æ–‡ä»¶åˆ†å‘æƒ…å†µ

```bash
[atguigu@hadoop103 ~]$ cat /opt/module/hadoop-3.1.3/etc/hadoop/core-site.xml
[atguigu@hadoop104 ~]$ cat /opt/module/hadoop-3.1.3/etc/hadoop/core-site.xml
```



#### 3.2.5 ç¾¤èµ·é›†ç¾¤

- é…ç½® workers

```bash
[atguigu@hadoop102 hadoop]$ vim /opt/module/hadoop-3.1.3/etc/hadoop/workers

# åœ¨è¯¥æ–‡ä»¶ä¸­å¢åŠ å¦‚ä¸‹å†…å®¹ï¼š
hadoop102
hadoop103
hadoop104

# åŒæ­¥æ‰€æœ‰èŠ‚ç‚¹é…ç½®æ–‡ä»¶
[atguigu@hadoop102 hadoop]$ xsync /opt/module/hadoop-3.1.3/etc
```

- å¯åŠ¨é›†ç¾¤

  - **å¦‚æœé›†ç¾¤æ˜¯ç¬¬ä¸€æ¬¡å¯åŠ¨**ï¼Œéœ€è¦åœ¨ hadoop102 èŠ‚ç‚¹æ ¼å¼åŒ– `NameNode`

  ```bash
  [atguigu@hadoop102 hadoop-3.1.3]$ hdfs namenode -format
  ```

  - å¯åŠ¨ HDFS

  > å‚è€ƒï¼šhttps://blog.csdn.net/weixin_43848614/article/details/112596493

  ```bash
  [atguigu@hadoop102 hadoop-3.1.3]$ sbin/start-dfs.sh
  ```

  

  ![](https://notes2021.oss-cn-beijing.aliyuncs.com/2021/image-20220825002900933.png)

  

  - åœ¨é…ç½®äº† ResourceManager çš„èŠ‚ç‚¹ï¼ˆhadoop103ï¼‰å¯åŠ¨YARN

  ```bash
  [atguigu@hadoop103 hadoop-3.1.3]$ sbin/start-yarn.sh
  ```

  - Web ç«¯æŸ¥çœ‹ HDFS çš„ NameNode
    - æµè§ˆå™¨ä¸­è¾“å…¥ï¼šhttp://hadoop102:9870
    - æŸ¥çœ‹ HDFS ä¸Šå­˜å‚¨çš„æ•°æ®ä¿¡æ¯

  ![](https://notes2021.oss-cn-beijing.aliyuncs.com/2021/image-20220825003952217.png)

  

  - Web ç«¯æŸ¥çœ‹ YARN çš„ ResourceManager
    - æµè§ˆå™¨ä¸­è¾“å…¥ï¼šhttp://hadoop103:8088
    - æŸ¥çœ‹ YARN ä¸Šè¿è¡Œçš„ Job ä¿¡æ¯

  ![](https://notes2021.oss-cn-beijing.aliyuncs.com/2021/image-20220825004009021.png)

- é›†ç¾¤åŸºæœ¬æµ‹è¯•

  - ä¸Šä¼ æ–‡ä»¶åˆ°é›†ç¾¤

  ```bash
  # ä¸Šä¼ å°æ–‡ä»¶
  [atguigu@hadoop102 ~]$ hadoop fs -mkdir /input
  [atguigu@hadoop102 ~]$ hadoop fs -put $HADOOP_HOME/wcinput/word.txt /input
  
  # ä¸Šä¼ å¤§æ–‡ä»¶
  [atguigu@hadoop102 ~]$ hadoop fs -put  /opt/software/jdk-8u212-linux-x64.tar.gz  /
  ```

  - ä¸Šä¼ æ–‡ä»¶åæŸ¥çœ‹æ–‡ä»¶å­˜æ”¾åœ¨ä»€ä¹ˆä½ç½®

  ```bash
  # æŸ¥çœ‹HDFSæ–‡ä»¶å­˜å‚¨è·¯å¾„
  [atguigu@hadoop102 subdir0]$ pwd
  /opt/module/hadoop-3.1.3/data/dfs/data/current/BP-1436128598-192.168.10.102-1610603650062/current/finalized/subdir0/subdir0
  
  # æŸ¥çœ‹HDFSåœ¨ç£ç›˜å­˜å‚¨æ–‡ä»¶å†…å®¹
  [atguigu@hadoop102 subdir0]$ cat blk_1073741825
  hadoop yarn
  hadoop mapreduce 
  atguigu
  atguigu
  ```

  - æ‹¼æ¥

  ```bash
  -rw-rw-r--. 1 atguigu atguigu 134217728 5æœˆ  23 16:01 blk_1073741836
  -rw-rw-r--. 1 atguigu atguigu   1048583 5æœˆ  23 16:01 blk_1073741836_1012.meta
  -rw-rw-r--. 1 atguigu atguigu  63439959 5æœˆ  23 16:01 blk_1073741837
  -rw-rw-r--. 1 atguigu atguigu    495635 5æœˆ  23 16:01 blk_1073741837_1013.meta
  [atguigu@hadoop102 subdir0]$ cat blk_1073741836>>tmp.tar.gz
  [atguigu@hadoop102 subdir0]$ cat blk_1073741837>>tmp.tar.gz
  [atguigu@hadoop102 subdir0]$ tar -zxvf tmp.tar.gz
  ```

  

  - ä¸‹è½½

  ```bash
  [atguigu@hadoop104 software]$ hadoop fs -get /jdk-8u212-linux-x64.tar.gz ./
  ```

  - æ‰§è¡Œ `wordcount` ç¨‹åº

  ```bash
  [atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output
  ```

#### 3.2.6 é…ç½®å†å²æœåŠ¡å™¨

ä¸ºäº†æŸ¥çœ‹ç¨‹åºçš„å†å²è¿è¡Œæƒ…å†µï¼Œéœ€è¦é…ç½®ä¸€ä¸‹å†å²æœåŠ¡å™¨ã€‚

- é…ç½® `mapred-site.xml`

```bash
[atguigu@hadoop102 hadoop]$ vim mapred-site.xml
```

å¢åŠ å¦‚ä¸‹é…ç½®

```xml
<!-- å†å²æœåŠ¡å™¨ç«¯åœ°å€ -->
<property>
    <name>mapreduce.jobhistory.address</name>
    <value>hadoop102:10020</value>
</property>

<!-- å†å²æœåŠ¡å™¨webç«¯åœ°å€ -->
<property>
    <name>mapreduce.jobhistory.webapp.address</name>
    <value>hadoop102:19888</value>
</property>
```



- åˆ†å‘é…ç½®

```bash
[atguigu@hadoop102 hadoop]$ xsync $HADOOP_HOME/etc/hadoop/mapred-site.xml
```



- åœ¨ hadoop102 å¯åŠ¨å†å²æœåŠ¡å™¨

```bash
[atguigu@hadoop102 hadoop]$ mapred --daemon start historyserver
```



- æŸ¥çœ‹å†å²æœåŠ¡å™¨æ˜¯å¦å¯åŠ¨

```bash
[atguigu@hadoop102 hadoop]$ jps
```



- æŸ¥çœ‹ JobHistoryï¼šhttp://hadoop102:19888/jobhistory



#### 3.2.7 é…ç½®æ—¥å¿—çš„èšé›†

- æ—¥å¿—èšé›†æ¦‚å¿µï¼šåº”ç”¨è¿è¡Œå®Œæˆä»¥åï¼Œå°†ç¨‹åºè¿è¡Œæ—¥å¿—ä¿¡æ¯ä¸Šä¼ åˆ° HDFS ç³»ç»Ÿä¸Šã€‚
  - å¥½å¤„ï¼šå¯ä»¥æ–¹ä¾¿çš„æŸ¥çœ‹åˆ°ç¨‹åºè¿è¡Œè¯¦æƒ…ï¼Œæ–¹ä¾¿å¼€å‘è°ƒè¯•ã€‚
  - æ³¨æ„ï¼šå¼€å¯æ—¥å¿—èšé›†åŠŸèƒ½ï¼Œéœ€è¦é‡æ–°å¯åŠ¨ NodeManager ã€ResourceManager å’Œ HistoryServerã€‚

- å¼€å¯

  - é…ç½® `yarn-site.xml`

  ```bash
  [atguigu@hadoop102 hadoop]$ vim yarn-site.xml
  ```

  å¢åŠ å¦‚ä¸‹é…ç½®

  ```xml
  <!-- å¼€å¯æ—¥å¿—èšé›†åŠŸèƒ½ -->
  <property>
      <name>yarn.log-aggregation-enable</name>
      <value>true</value>
  </property>
  <!-- è®¾ç½®æ—¥å¿—èšé›†æœåŠ¡å™¨åœ°å€ -->
  <property>  
      <name>yarn.log.server.url</name>  
      <value>http://hadoop102:19888/jobhistory/logs</value>
  </property>
  <!-- è®¾ç½®æ—¥å¿—ä¿ç•™æ—¶é—´ä¸º7å¤© -->
  <property>
      <name>yarn.log-aggregation.retain-seconds</name>
      <value>604800</value>
  </property>    
  ```

  - åˆ†å‘é…ç½®

  ```bash
  [atguigu@hadoop102 hadoop]$ xsync $HADOOP_HOME/etc/hadoop/yarn-site.xml
  ```

  - å…³é—­NodeManager ã€ResourceManager å’Œ HistoryServer

  ```bash
  [atguigu@hadoop103 hadoop-3.1.3]$ sbin/stop-yarn.sh
  [atguigu@hadoop103 hadoop-3.1.3]$ mapred --daemon stop historyserver
  ```

  - å¯åŠ¨NodeManager ã€ResourceManageå’ŒHistoryServer

  ```bash
  [atguigu@hadoop103 ~]$ start-yarn.sh
  [atguigu@hadoop102 ~]$ mapred --daemon start historyserver
  ```

  - åˆ é™¤HDFSä¸Šå·²ç»å­˜åœ¨çš„è¾“å‡ºæ–‡ä»¶

  ```bash
  [atguigu@hadoop102 ~]$ hadoop fs -rm -r /output
  ```

  - æ‰§è¡ŒWordCountç¨‹åº

  ```bash
  [atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output
  ```

  - æŸ¥çœ‹æ—¥å¿—
    - å†å²æœåŠ¡å™¨åœ°å€ï¼šhttp://hadoop102:19888/jobhistory
    - å†å²ä»»åŠ¡åˆ—è¡¨
    - æŸ¥çœ‹ä»»åŠ¡è¿è¡Œæ—¥å¿—
    - è¿è¡Œæ—¥å¿—è¯¦æƒ…



![](https://notes2021.oss-cn-beijing.aliyuncs.com/2021/image-20220825231327179.png)



#### 3.2.8 é›†ç¾¤å¯åŠ¨/åœæ­¢æ–¹å¼æ€»ç»“

- å„ä¸ªæ¨¡å—åˆ†å¼€å¯åŠ¨/åœæ­¢ï¼ˆé…ç½® ssh æ˜¯å‰æï¼‰ å¸¸ç”¨

```bash
# æ•´ä½“å¯åŠ¨/åœæ­¢HDFS
start-dfs.sh/stop-dfs.sh

# æ•´ä½“å¯åŠ¨/åœæ­¢YARN
start-yarn.sh/stop-yarn.sh
```

- å„ä¸ªæœåŠ¡ç»„ä»¶é€ä¸€å¯åŠ¨/åœæ­¢

```bash
# åˆ†åˆ«å¯åŠ¨/åœæ­¢HDFSç»„ä»¶
hdfs --daemon start/stop namenode/datanode/secondarynamenode

# å¯åŠ¨/åœæ­¢YARN
yarn --daemon start/stop  resourcemanager/nodemanager
```



#### 3.2.9 ç¼–å†™Hadoopé›†ç¾¤å¸¸ç”¨è„šæœ¬

- Hadoopé›†ç¾¤å¯åœè„šæœ¬ `myhadoop.sh` ï¼ˆåŒ…å«HDFSï¼ŒYarnï¼ŒHistoryserverï¼‰

```bash
[atguigu@hadoop102 ~]$ cd /home/atguigu/bin
[atguigu@hadoop102 bin]$ vim myhadoop.sh
```

è¾“å…¥å¦‚ä¸‹å†…å®¹ï¼š

```sh
#!/bin/bash

if [ $# -lt 1 ]
then
    echo "No Args Input..."
    exit ;
fi

case $1 in
"start")
        echo " =================== å¯åŠ¨ hadoopé›†ç¾¤ ==================="

        echo " --------------- å¯åŠ¨ hdfs ---------------"
        ssh hadoop102 "/opt/module/hadoop-3.1.3/sbin/start-dfs.sh"
        echo " --------------- å¯åŠ¨ yarn ---------------"
        ssh hadoop103 "/opt/module/hadoop-3.1.3/sbin/start-yarn.sh"
        echo " --------------- å¯åŠ¨ historyserver ---------------"
        ssh hadoop102 "/opt/module/hadoop-3.1.3/bin/mapred --daemon start historyserver"
;;
"stop")
        echo " =================== å…³é—­ hadoopé›†ç¾¤ ==================="

        echo " --------------- å…³é—­ historyserver ---------------"
        ssh hadoop102 "/opt/module/hadoop-3.1.3/bin/mapred --daemon stop historyserver"
        echo " --------------- å…³é—­ yarn ---------------"
        ssh hadoop103 "/opt/module/hadoop-3.1.3/sbin/stop-yarn.sh"
        echo " --------------- å…³é—­ hdfs ---------------"
        ssh hadoop102 "/opt/module/hadoop-3.1.3/sbin/stop-dfs.sh"
;;
*)
    echo "Input Args Error..."
;;
esac
```

ä¿å­˜åé€€å‡ºï¼Œç„¶åèµ‹äºˆè„šæœ¬æ‰§è¡Œæƒé™

```bash
[atguigu@hadoop102 bin]$ chmod +x myhadoop.sh
```

- æŸ¥çœ‹ä¸‰å°æœåŠ¡å™¨Javaè¿›ç¨‹è„šæœ¬ï¼šjpsall

```bash
[atguigu@hadoop102 ~]$ cd /home/atguigu/bin
[atguigu@hadoop102 bin]$ vim jpsall
```

è¾“å…¥å¦‚ä¸‹å†…å®¹ï¼š

```bash
#!/bin/bash

for host in hadoop102 hadoop103 hadoop104
do
        echo =============== $host ===============
        ssh $host jps 
done
```

ä¿å­˜åé€€å‡ºï¼Œç„¶åèµ‹äºˆè„šæœ¬æ‰§è¡Œæƒé™

```bash
[atguigu@hadoop102 bin]$ chmod +x jpsall
```

- åˆ†å‘ /home/atguigu/bin ç›®å½•ï¼Œä¿è¯è‡ªå®šä¹‰è„šæœ¬åœ¨ä¸‰å°æœºå™¨ä¸Šéƒ½å¯ä»¥ä½¿ç”¨

```bash
[atguigu@hadoop102 ~]$ xsync /home/atguigu/bin/
```



![](https://notes2021.oss-cn-beijing.aliyuncs.com/2021/image-20220825231040057.png)



#### 3.2.10 å¸¸ç”¨ç«¯å£å·è¯´æ˜

| ç«¯å£åç§°                  | Hadoop2.x   | Hadoop3.x            |
| ------------------------- | ----------- | -------------------- |
| NameNodeå†…éƒ¨é€šä¿¡ç«¯å£      | 8020 / 9000 | 8020 ğŸˆ / 9000 / 9820 |
| NameNode HTTP UI          | 50070 ğŸˆ     | 9870 ğŸˆ               |
| MapReduceæŸ¥çœ‹æ‰§è¡Œä»»åŠ¡ç«¯å£ | 8088        | 8088                 |
| å†å²æœåŠ¡å™¨é€šä¿¡ç«¯å£        | 19888       | 19888                |



#### 3.2.11 é›†ç¾¤æ—¶é—´åŒæ­¥

- **å¦‚æœæœåŠ¡å™¨åœ¨å…¬ç½‘ç¯å¢ƒï¼ˆèƒ½è¿æ¥å¤–ç½‘ï¼‰ï¼Œå¯ä»¥ä¸é‡‡ç”¨é›†ç¾¤æ—¶é—´åŒæ­¥**ï¼Œå› ä¸ºæœåŠ¡å™¨ä¼šå®šæœŸå’Œå…¬ç½‘æ—¶é—´è¿›è¡Œæ ¡å‡†ï¼›

- å¦‚æœæœåŠ¡å™¨åœ¨å†…ç½‘ç¯å¢ƒï¼Œå¿…é¡»è¦é…ç½®é›†ç¾¤æ—¶é—´åŒæ­¥ï¼Œå¦åˆ™æ—¶é—´ä¹…äº†ï¼Œä¼šäº§ç”Ÿæ—¶é—´åå·®ï¼Œå¯¼è‡´é›†ç¾¤æ‰§è¡Œä»»åŠ¡æ—¶é—´ä¸åŒæ­¥ã€‚

- éœ€æ±‚
- æ—¶é—´æœåŠ¡å™¨é…ç½®ï¼ˆå¿…é¡»rootç”¨æˆ·ï¼‰
- å…¶ä»–æœºå™¨é…ç½®ï¼ˆå¿…é¡»rootç”¨æˆ·ï¼‰



## 4 å¸¸è§é”™è¯¯åŠè§£å†³æ–¹æ¡ˆ

- 1ã€é˜²ç«å¢™æ²¡å…³é—­ã€æˆ–è€…æ²¡æœ‰å¯åŠ¨YARN
  - *INFO client.RMProxy: Connecting to ResourceManager at hadoop108/192.168.10.108:8032*

- 2ã€ä¸»æœºåç§°é…ç½®é”™è¯¯

- 3ã€IP åœ°å€é…ç½®é”™è¯¯

- 4ã€ssh æ²¡æœ‰é…ç½®å¥½

- 5ã€root ç”¨æˆ·å’Œ atguigu ä¸¤ä¸ªç”¨æˆ·å¯åŠ¨é›†ç¾¤ä¸ç»Ÿä¸€

- 6ã€é…ç½®æ–‡ä»¶ä¿®æ”¹ä¸ç»†å¿ƒ

- 7ã€ä¸è¯†åˆ«ä¸»æœºåç§°

  ```bash
  java.net.UnknownHostException: hadoop102: hadoop102
          at java.net.InetAddress.getLocalHost(InetAddress.java:1475)
          at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:146)
          at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1290)
          at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1287)
          at java.security.AccessController.doPrivileged(Native Method)
  at javax.security.auth.Subject.doAs(Subject.java:415)
  ```

  - è§£å†³åŠæ³•ï¼š

    ï¼ˆ1ï¼‰åœ¨ `/etc/hosts` æ–‡ä»¶ä¸­æ·»åŠ 192.168.10.102 hadoop102

    ï¼ˆ2ï¼‰ä¸»æœºåç§°ä¸è¦èµ· hadoop  hadoop000 ç­‰ç‰¹æ®Šåç§°

- 8ã€DataNode å’Œ NameNodeè¿›ç¨‹åŒæ—¶åªèƒ½å·¥ä½œä¸€ä¸ªã€‚
- 9ã€æ‰§è¡Œå‘½ä»¤ä¸ç”Ÿæ•ˆï¼Œç²˜è´´Wordä¸­å‘½ä»¤æ—¶ï¼Œé‡åˆ°-å’Œé•¿â€“æ²¡åŒºåˆ†å¼€ã€‚å¯¼è‡´å‘½ä»¤å¤±æ•ˆ
  - è§£å†³åŠæ³•ï¼šå°½é‡ä¸è¦ç²˜è´´Wordä¸­ä»£ç ã€‚

- 10ã€jps å‘ç°è¿›ç¨‹å·²ç»æ²¡æœ‰ï¼Œä½†æ˜¯é‡æ–°å¯åŠ¨é›†ç¾¤ï¼Œæç¤ºè¿›ç¨‹å·²ç»å¼€å¯ã€‚
  
  - åŸå› æ˜¯åœ¨ Linux çš„æ ¹ç›®å½•ä¸‹ /tmp ç›®å½•ä¸­å­˜åœ¨å¯åŠ¨çš„è¿›ç¨‹ä¸´æ—¶æ–‡ä»¶ï¼Œå°†é›†ç¾¤ç›¸å…³è¿›ç¨‹åˆ é™¤æ‰ï¼Œå†é‡æ–°å¯åŠ¨é›†ç¾¤ã€‚
  
- 11ã€jps ä¸ç”Ÿæ•ˆ âœ”
  
  - åŸå› ï¼šå…¨å±€å˜é‡ `hadoop java` æ²¡æœ‰ç”Ÿæ•ˆã€‚
  - è§£å†³åŠæ³•ï¼šéœ€è¦ `source /etc/profile` æ–‡ä»¶ã€‚
  
- 12ã€8088ç«¯å£è¿æ¥ä¸ä¸Š



# HDFS

## 5 HDFS æ¦‚è¿°

### 5.1HDFS äº§ç”ŸèƒŒæ™¯ã€å®šä¹‰å’Œä½¿ç”¨åœºæ™¯

- äº§ç”ŸèƒŒæ™¯
  - éšç€æ•°æ®é‡è¶Šæ¥è¶Šå¤§ï¼Œåœ¨ä¸€ä¸ªæ“ä½œç³»ç»Ÿå­˜ä¸ä¸‹æ‰€æœ‰çš„æ•°æ®ï¼›
  - é‚£ä¹ˆå°±åˆ†é…åˆ°æ›´å¤šçš„æ“ä½œç³»ç»Ÿç®¡ç†çš„ç£ç›˜ä¸­ï¼Œä½†æ˜¯ä¸æ–¹ä¾¿ç®¡ç†å’Œç»´æŠ¤ï¼Œè¿«åˆ‡éœ€è¦ä¸€ç§ç³»ç»Ÿæ¥ç®¡ç†å¤šå°æœºå™¨ä¸Šçš„æ–‡ä»¶ï¼Œè¿™å°±æ˜¯åˆ†å¸ƒå¼æ–‡ä»¶ç®¡ç†ç³»ç»Ÿã€‚
  - HDFS åªæ˜¯**åˆ†å¸ƒå¼æ–‡ä»¶ç®¡ç†ç³»ç»Ÿ**ä¸­çš„ä¸€ç§ã€‚

- å®šä¹‰
  - HDFSï¼ˆHadoop Distributed File Systemï¼‰ï¼Œå®ƒæ˜¯ä¸€ä¸ªæ–‡ä»¶ç³»ç»Ÿï¼Œç”¨äºå­˜å‚¨æ–‡ä»¶ï¼Œé€šè¿‡ç›®å½•æ ‘æ¥å®šä½æ–‡ä»¶ï¼›
  - å…¶æ¬¡ï¼Œå®ƒæ˜¯åˆ†å¸ƒå¼çš„ï¼Œç”±å¾ˆå¤šæœåŠ¡å™¨è”åˆèµ·æ¥å®ç°å…¶åŠŸèƒ½ï¼Œé›†ç¾¤ä¸­çš„æœåŠ¡å™¨æœ‰å„è‡ªçš„è§’è‰²ã€‚

- ä½¿ç”¨åœºæ™¯
  - **é€‚åˆä¸€æ¬¡å†™å…¥ï¼Œå¤šæ¬¡è¯»å‡ºçš„åœºæ™¯**ã€‚ä¸€ä¸ªæ–‡ä»¶ç»è¿‡åˆ›å»ºã€å†™å…¥å’Œå…³é—­ä¹‹åå°±ä¸éœ€è¦æ”¹å˜ã€‚

### 5.2HDFS ä¼˜ç¼ºç‚¹

- ä¼˜ç‚¹
  - é«˜å®¹é”™æ€§ï¼šæ•°æ®è‡ªåŠ¨ä¿å­˜å¤šä¸ªå‰¯æœ¬ã€‚æŸä¸€ä¸ªå‰¯æœ¬ä¸¢å¤±ä»¥åï¼Œå¯ä»¥è‡ªåŠ¨æ¢å¤ã€‚
  - é€‚åˆå¤„ç†å¤§æ•°æ®
    - æ•°æ®è§„æ¨¡ï¼šèƒ½å¤Ÿå¤„ç†æ•°æ®è§„æ¨¡è¾¾åˆ°GBã€TBç”šè‡³**PBçº§åˆ«çš„æ•°æ®**ï¼›
    - æ–‡ä»¶è§„æ¨¡ï¼šèƒ½å¤Ÿå¤„ç†**ç™¾ä¸‡**è§„æ¨¡ä»¥ä¸Šçš„**æ–‡ä»¶æ•°é‡**ï¼Œæ•°é‡ç›¸å½“ä¹‹å¤§ã€‚
  - å¯æ„å»ºåœ¨å»‰ä»·æœºå™¨ä¸Šï¼Œé€šè¿‡å¤šå‰¯æœ¬æœºåˆ¶ï¼Œæé«˜å¯é æ€§ã€‚
- ç¼ºç‚¹
  - ä¸é€‚åˆä½å»¶æ—¶æ•°æ®è®¿é—®ï¼ˆæ¯”å¦‚æ¯«ç§’çº§çš„å­˜å‚¨æ•°æ®ï¼‰
  - æ— æ³•é«˜æ•ˆåœ°å¯¹å¤§é‡å°æ–‡ä»¶è¿›è¡Œå­˜å‚¨ï¼ˆå¯»å€æ—¶é—´ä¼šè¶…è¿‡è¯»å–æ—¶é—´ï¼‰
  - ä¸æ”¯æŒå¹¶å‘å†™å…¥ï¼ˆä¸å…è®¸å¤šä¸ªæ–‡ä»¶åŒæ—¶å†™ï¼‰ã€ä¸æ”¯æŒæ–‡ä»¶éšæœºä¿®æ”¹ï¼ˆä»…æ”¯æŒæ•°æ®appendè¿½åŠ ï¼‰



### 5.3HDFS ç»„æˆæ¶æ„





### 5.4HDFS æ–‡ä»¶å—å¤§å°ï¼ˆé¢è¯•é‡ç‚¹ï¼‰

- HDFS ä¸­çš„æ–‡ä»¶åœ¨ç‰©ç†ä¸Šæ˜¯åˆ†å—å­˜å‚¨ï¼ˆBlockï¼‰ï¼Œå—çš„å¤§å°å¯ä»¥é€šè¿‡é…ç½®å‚æ•°æ¥è§„å®šï¼Œé»˜è®¤å¤§å°åœ¨Hadoop2.x/3.xç‰ˆæœ¬æ˜¯128Mã€‚

- ä¸èƒ½è®¾ç½®å¤ªå°ï¼Œä¹Ÿä¸èƒ½è®¾ç½®å¤ªå¤§ï¼Ÿ
  - å¤ªå°ï¼Œä¼šå¢åŠ å¯»å€æ—¶é—´
  - å¤ªå¤§ï¼Œç¨‹åºåœ¨å¤„ç†è¿™å—æ•°æ®æ—¶ï¼Œä¼šéå¸¸æ…¢
  - HDFSå—çš„å¤§å°è®¾ç½®ä¸»è¦å–å†³äºç£ç›˜ä¼ è¾“é€Ÿç‡ã€‚



## 6 HDFS çš„shellæ“ä½œï¼ˆå¼€å‘é‡ç‚¹ï¼‰

### 6.1åŸºæœ¬è¯­æ³•

- hadoop fs å…·ä½“å‘½ä»¤
- hdfs dfs å…·ä½“å‘½ä»¤



### 6.2å‘½ä»¤å¤§å…¨

```bash
[atguigu@hadoop102 hadoop-3.1.3]$ bin/hadoop fs
```



### 6.3å¸¸ç”¨å‘½ä»¤å®æ“

#### å‡†å¤‡å·¥ä½œ

- å¯åŠ¨Hadoopé›†ç¾¤ï¼ˆæ–¹ä¾¿åç»­çš„æµ‹è¯•ï¼‰

```bash
[atguigu@hadoop102 hadoop-3.1.3]$ sbin/start-dfs.sh
[atguigu@hadoop103 hadoop-3.1.3]$ sbin/start-yarn.sh
```



- -helpï¼šè¾“å‡ºè¿™ä¸ªå‘½ä»¤å‚æ•°

```bash
[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -help rm
```



- åˆ›å»º/sanguoæ–‡ä»¶å¤¹

```bash
[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -mkdir /sanguo
```



#### ä¸Šä¼ 

- -moveFromLocalï¼šä»æœ¬åœ°å‰ªåˆ‡ç²˜è´´åˆ° HDFS

```bash
[atguigu@hadoop102 hadoop-3.1.3]$ vim shuguo.txt
è¾“å…¥ï¼š
shuguo

[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs  -moveFromLocal  ./shuguo.txt  /sanguo
```

- -copyFromLocalï¼šä»æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿä¸­æ‹·è´æ–‡ä»¶åˆ° HDFS è·¯å¾„å»

```bash
[atguigu@hadoop102 hadoop-3.1.3]$ vim weiguo.txt
è¾“å…¥ï¼š
weiguo

[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -copyFromLocal weiguo.txt /sanguo
```

- -putï¼šç­‰åŒäºcopyFromLocalï¼Œç”Ÿäº§ç¯å¢ƒæ›´ä¹ æƒ¯ç”¨put

```bash
[atguigu@hadoop102 hadoop-3.1.3]$ vim wuguo.txt
è¾“å…¥ï¼š
wuguo

[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -put ./wuguo.txt /sanguo
```

- -appendToFileï¼šè¿½åŠ ä¸€ä¸ªæ–‡ä»¶åˆ°å·²ç»å­˜åœ¨çš„æ–‡ä»¶æœ«å°¾

```bash
[atguigu@hadoop102 hadoop-3.1.3]$ vim liubei.txt
è¾“å…¥ï¼š
liubei

[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -appendToFile liubei.txt /sanguo/shuguo.txt
```



#### ä¸‹è½½

- -copyToLocalï¼šä»HDFSæ‹·è´åˆ°æœ¬åœ°

```bash
[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -copyToLocal /sanguo/shuguo.txt ./
```

- -getï¼šç­‰åŒäºcopyToLocalï¼Œç”Ÿäº§ç¯å¢ƒæ›´ä¹ æƒ¯ç”¨get

```bash
[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -get /sanguo/shuguo.txt ./shuguo2.txt
```

#### ç›´æ¥æ“ä½œ

```bash
-ls: æ˜¾ç¤ºç›®å½•ä¿¡æ¯
[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -ls /sanguo

-catï¼šæ˜¾ç¤ºæ–‡ä»¶å†…å®¹
[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -cat /sanguo/shuguo.txt

-chgrpã€-chmodã€-chownï¼šLinuxæ–‡ä»¶ç³»ç»Ÿä¸­çš„ç”¨æ³•ä¸€æ ·ï¼Œä¿®æ”¹æ–‡ä»¶æ‰€å±æƒé™
[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs  -chmod 666  /sanguo/shuguo.txt
[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs  -chown  atguigu:atguigu   /sanguo/shuguo.txt

-mkdirï¼šåˆ›å»ºè·¯å¾„
[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -mkdir /jinguo

-cpï¼šä»HDFSçš„ä¸€ä¸ªè·¯å¾„æ‹·è´åˆ°HDFSçš„å¦ä¸€ä¸ªè·¯å¾„
[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -cp /sanguo/shuguo.txt /jinguo

-mvï¼šåœ¨HDFSç›®å½•ä¸­ç§»åŠ¨æ–‡ä»¶
[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -mv /sanguo/wuguo.txt /jinguo
[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -mv /sanguo/weiguo.txt /jinguo

-tailï¼šæ˜¾ç¤ºä¸€ä¸ªæ–‡ä»¶çš„æœ«å°¾1kbçš„æ•°æ®
[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -tail /jinguo/shuguo.txt

-rmï¼šåˆ é™¤æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹
[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -rm /sanguo/shuguo.txt

-duç»Ÿè®¡æ–‡ä»¶å¤¹çš„å¤§å°ä¿¡æ¯
[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -du -s -h /jinguo
27  81  /jinguo

[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -du  -h /jinguo
14  42  /jinguo/shuguo.txt
7   21   /jinguo/weiguo.txt
6   18   /jinguo/wuguo.tx
è¯´æ˜ï¼š27è¡¨ç¤ºæ–‡ä»¶å¤§å°ï¼›81è¡¨ç¤º27*3ä¸ªå‰¯æœ¬ï¼›/jinguoè¡¨ç¤ºæŸ¥çœ‹çš„ç›®å½•

-setrepï¼šè®¾ç½®HDFSä¸­æ–‡ä»¶çš„å‰¯æœ¬æ•°é‡
[atguigu@hadoop102 hadoop-3.1.3]$ hadoop fs -setrep 10 /jinguo/shuguo.txt

# æ³¨ï¼šè¿™é‡Œè®¾ç½®çš„å‰¯æœ¬æ•°åªæ˜¯è®°å½•åœ¨NameNodeçš„å…ƒæ•°æ®ä¸­ï¼Œæ˜¯å¦çœŸçš„ä¼šæœ‰è¿™ä¹ˆå¤šå‰¯æœ¬ï¼Œè¿˜å¾—çœ‹DataNodeçš„æ•°é‡ã€‚
# å› ä¸ºç›®å‰åªæœ‰3å°è®¾å¤‡ï¼Œæœ€å¤šä¹Ÿå°±3ä¸ªå‰¯æœ¬ï¼Œåªæœ‰èŠ‚ç‚¹æ•°å¢åŠ åˆ°10å°æ—¶ï¼Œå‰¯æœ¬æ•°æ‰èƒ½è¾¾åˆ°10ã€‚
```



## 7 HDFS çš„APIæ“ä½œ

### 7.1å®¢æˆ·ç«¯ç¯å¢ƒå‡†å¤‡

- æ‰¾åˆ°èµ„æ–™åŒ…è·¯å¾„ä¸‹çš„ Windos ä¾èµ–æ–‡ä»¶å¤¹ï¼Œæ‹·è´hadoop-3.1.0åˆ°éä¸­æ–‡è·¯å¾„ï¼ˆæ¯”å¦‚d:\ï¼‰ã€‚

- é…ç½® `HADOOP_HOME` ç¯å¢ƒå˜é‡

- é…ç½®Pathç¯å¢ƒå˜é‡

> éªŒè¯Hadoopç¯å¢ƒå˜é‡æ˜¯å¦æ­£å¸¸ã€‚åŒå‡»winutils.exeï¼Œå¦‚æœæŠ¥å¦‚ä¸‹é”™è¯¯ã€‚è¯´æ˜ç¼ºå°‘å¾®è½¯è¿è¡Œåº“ï¼ˆæ­£ç‰ˆç³»ç»Ÿå¾€å¾€æœ‰è¿™ä¸ªé—®é¢˜ï¼‰ã€‚åœ¨èµ„æ–™åŒ…é‡Œé¢æœ‰å¯¹åº”çš„å¾®è½¯è¿è¡Œåº“å®‰è£…åŒ…åŒå‡»å®‰è£…å³å¯ã€‚

- åœ¨IDEAä¸­åˆ›å»ºä¸€ä¸ªMavenå·¥ç¨‹ HdfsClientDemoï¼Œå¹¶å¯¼å…¥ç›¸åº”çš„ä¾èµ–åæ ‡+æ—¥å¿—æ·»åŠ 

```xml
<dependencies>
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-client</artifactId>
        <version>3.1.3</version>
    </dependency>
    
    <dependency>
        <groupId>junit</groupId>
        <artifactId>junit</artifactId>
        <version>4.12</version>
    </dependency>
    
    <dependency>
        <groupId>org.slf4j</groupId>
        <artifactId>slf4j-log4j12</artifactId>
        <version>1.7.30</version>
    </dependency>
</dependencies>
```

- åœ¨é¡¹ç›®çš„ `src/main/resources` ç›®å½•ä¸‹ï¼Œæ–°å»ºä¸€ä¸ªæ–‡ä»¶ï¼Œå‘½åä¸ºâ€œlog4j.propertiesâ€ï¼Œåœ¨æ–‡ä»¶ä¸­å¡«å…¥

```properties
log4j.rootLogger=INFO, stdout  
log4j.appender.stdout=org.apache.log4j.ConsoleAppender  
log4j.appender.stdout.layout=org.apache.log4j.PatternLayout  
log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n  
log4j.appender.logfile=org.apache.log4j.FileAppender  
log4j.appender.logfile.File=target/spring.log  
log4j.appender.logfile.layout=org.apache.log4j.PatternLayout  
log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n
```

- åˆ›å»ºåŒ…åï¼šcom.atguigu.hdfs
- åˆ›å»º HdfsClient ç±»

```java
public class HdfsClient {

    @Test
    public void testMkdirs() throws IOException, URISyntaxException, InterruptedException {

        // 1 è·å–æ–‡ä»¶ç³»ç»Ÿ
        Configuration configuration = new Configuration();

        // FileSystem fs = FileSystem.get(new URI("hdfs://hadoop102:8020"), configuration);
        FileSystem fs = FileSystem.get(new URI("hdfs://hadoop102:8020"), configuration,"atguigu");

        // 2 åˆ›å»ºç›®å½•
        fs.mkdirs(new Path("/xiyou/huaguoshan/"));

        // 3 å…³é—­èµ„æº
        fs.close();
    }
}
```

- æ‰§è¡Œç¨‹åº

å®¢æˆ·ç«¯å»æ“ä½œHDFSæ—¶ï¼Œæ˜¯æœ‰ä¸€ä¸ªç”¨æˆ·èº«ä»½çš„ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼ŒHDFSå®¢æˆ·ç«¯APIä¼šä»é‡‡ç”¨Windowsé»˜è®¤ç”¨æˆ·è®¿é—®HDFSï¼Œä¼šæŠ¥æƒé™å¼‚å¸¸é”™è¯¯ã€‚æ‰€ä»¥åœ¨è®¿é—®HDFSæ—¶ï¼Œä¸€å®šè¦é…ç½®ç”¨æˆ·ã€‚

```bash
org.apache.hadoop.security.AccessControlException: Permission denied: user=56576, access=WRITE, inode="/xiyou/huaguoshan":atguigu:supergroup:drwxr-xr-x
```



### 7.2HDFS çš„APIæ¡ˆä¾‹æ“ä½œ

#### HDFSæ–‡ä»¶ä¸Šä¼ ï¼ˆæµ‹è¯•å‚æ•°ä¼˜å…ˆçº§ï¼‰

- ç¼–å†™æºä»£ç 

```java
@Test
public void testCopyFromLocalFile() throws IOException, InterruptedException, URISyntaxException {

    // 1 è·å–æ–‡ä»¶ç³»ç»Ÿ
    Configuration configuration = new Configuration();
    configuration.set("dfs.replication", "2");
    FileSystem fs = FileSystem.get(new URI("hdfs://hadoop102:8020"), configuration, "atguigu");

    // 2 ä¸Šä¼ æ–‡ä»¶
    fs.copyFromLocalFile(new Path("d:/sunwukong.txt"), new Path("/xiyou/huaguoshan"));

    // 3 å…³é—­èµ„æº
    fs.close();
}
```

- å°†hdfs-site.xmlæ‹·è´åˆ°é¡¹ç›®çš„resourcesèµ„æºç›®å½•ä¸‹

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
	<property>
		<name>dfs.replication</name>
         <value>1</value>
	</property>
</configuration>
```

- å‚æ•°ä¼˜å…ˆçº§

å‚æ•°ä¼˜å…ˆçº§æ’åºï¼šï¼ˆ1ï¼‰å®¢æˆ·ç«¯ä»£ç ä¸­è®¾ç½®çš„å€¼ >ï¼ˆ2ï¼‰ClassPathä¸‹çš„ç”¨æˆ·è‡ªå®šä¹‰é…ç½®æ–‡ä»¶ >ï¼ˆ3ï¼‰ç„¶åæ˜¯æœåŠ¡å™¨çš„è‡ªå®šä¹‰é…ç½®ï¼ˆxxx-site.xmlï¼‰ >ï¼ˆ4ï¼‰æœåŠ¡å™¨çš„é»˜è®¤é…ç½®ï¼ˆxxx-default.xmlï¼‰



#### HDFSæ–‡ä»¶ä¸‹è½½

```java
@Test
public void testCopyToLocalFile() throws IOException, InterruptedException, URISyntaxException{

    // 1 è·å–æ–‡ä»¶ç³»ç»Ÿ
    Configuration configuration = new Configuration();
    FileSystem fs = FileSystem.get(new URI("hdfs://hadoop102:8020"), configuration, "atguigu");
    
    // 2 æ‰§è¡Œä¸‹è½½æ“ä½œ
    // boolean delSrc æŒ‡æ˜¯å¦å°†åŸæ–‡ä»¶åˆ é™¤
    // Path src æŒ‡è¦ä¸‹è½½çš„æ–‡ä»¶è·¯å¾„
    // Path dst æŒ‡å°†æ–‡ä»¶ä¸‹è½½åˆ°çš„è·¯å¾„
    // boolean useRawLocalFileSystem æ˜¯å¦å¼€å¯æ–‡ä»¶æ ¡éªŒ
    fs.copyToLocalFile(false, new Path("/xiyou/huaguoshan/sunwukong.txt"), new Path("d:/sunwukong2.txt"), true);
    
    // 3 å…³é—­èµ„æº
    fs.close();
}
```



#### HDFSæ–‡ä»¶æ›´åå’Œç§»åŠ¨

```java
@Test
public void testRename() throws IOException, InterruptedException, URISyntaxException{

	// 1 è·å–æ–‡ä»¶ç³»ç»Ÿ
	Configuration configuration = new Configuration();
	FileSystem fs = FileSystem.get(new URI("hdfs://hadoop102:8020"), configuration, "atguigu"); 
		
	// 2 ä¿®æ”¹æ–‡ä»¶åç§°
	fs.rename(new Path("/xiyou/huaguoshan/sunwukong.txt"), new Path("/xiyou/huaguoshan/meihouwang.txt"));
		
	// 3 å…³é—­èµ„æº
	fs.close();
}
```



#### HDFSåˆ é™¤æ–‡ä»¶å’Œç›®å½•

```java
@Test
public void testDelete() throws IOException, InterruptedException, URISyntaxException{

	// 1 è·å–æ–‡ä»¶ç³»ç»Ÿ
	Configuration configuration = new Configuration();
	FileSystem fs = FileSystem.get(new URI("hdfs://hadoop102:8020"), configuration, "atguigu");
		
	// 2 æ‰§è¡Œåˆ é™¤
	fs.delete(new Path("/xiyou"), true);
		
	// 3 å…³é—­èµ„æº
	fs.close();
}
```



#### HDFSæ–‡ä»¶è¯¦æƒ…æŸ¥çœ‹

æŸ¥çœ‹æ–‡ä»¶åç§°ã€æƒé™ã€é•¿åº¦ã€å—ä¿¡æ¯

```java
@Test
public void testListFiles() throws IOException, InterruptedException, URISyntaxException {

	// 1è·å–æ–‡ä»¶ç³»ç»Ÿ
	Configuration configuration = new Configuration();
	FileSystem fs = FileSystem.get(new URI("hdfs://hadoop102:8020"), configuration, "atguigu");

	// 2 è·å–æ–‡ä»¶è¯¦æƒ…
	RemoteIterator<LocatedFileStatus> listFiles = fs.listFiles(new Path("/"), true);

	while (listFiles.hasNext()) {
		LocatedFileStatus fileStatus = listFiles.next();

		System.out.println("========" + fileStatus.getPath() + "=========");
		System.out.println(fileStatus.getPermission());
		System.out.println(fileStatus.getOwner());
		System.out.println(fileStatus.getGroup());
		System.out.println(fileStatus.getLen());
		System.out.println(fileStatus.getModificationTime());
		System.out.println(fileStatus.getReplication());
		System.out.println(fileStatus.getBlockSize());
		System.out.println(fileStatus.getPath().getName());

		// è·å–å—ä¿¡æ¯
		BlockLocation[] blockLocations = fileStatus.getBlockLocations();
		System.out.println(Arrays.toString(blockLocations));
	}
	// 3 å…³é—­èµ„æº
	fs.close();
}
```



#### æ–‡ä»¶å’Œæ–‡ä»¶å¤¹åˆ¤æ–­

```java
@Test
public void testListStatus() throws IOException, InterruptedException, URISyntaxException{

    // 1 è·å–æ–‡ä»¶é…ç½®ä¿¡æ¯
    Configuration configuration = new Configuration();
    FileSystem fs = FileSystem.get(new URI("hdfs://hadoop102:8020"), configuration, "atguigu");

    // 2 åˆ¤æ–­æ˜¯æ–‡ä»¶è¿˜æ˜¯æ–‡ä»¶å¤¹
    FileStatus[] listStatus = fs.listStatus(new Path("/"));

    for (FileStatus fileStatus : listStatus) {

        // å¦‚æœæ˜¯æ–‡ä»¶
        if (fileStatus.isFile()) {
            System.out.println("f:"+fileStatus.getPath().getName());
        }else {
            System.out.println("d:"+fileStatus.getPath().getName());
        }
    }

    // 3 å…³é—­èµ„æº
    fs.close();
}
```



## 8 è¯»å†™æµç¨‹ï¼ˆé¢è¯•é‡ç‚¹ï¼‰

### 8.1HDFS å†™æ•°æ®æµç¨‹

#### åˆ¨ææ–‡ä»¶å†™å…¥

- ï¼ˆ1ï¼‰å®¢æˆ·ç«¯é€šè¿‡ Distributed FileSystem æ¨¡å—å‘ NameNode è¯·æ±‚ä¸Šä¼ æ–‡ä»¶ï¼ŒNameNode æ£€æŸ¥ç›®æ ‡æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼Œçˆ¶ç›®å½•æ˜¯å¦å­˜åœ¨ã€‚
- ï¼ˆ2ï¼‰NameNode è¿”å›æ˜¯å¦å¯ä»¥ä¸Šä¼ ã€‚
- ï¼ˆ3ï¼‰å®¢æˆ·ç«¯è¯·æ±‚ç¬¬ä¸€ä¸ª Block ä¸Šä¼ åˆ°å“ªå‡ ä¸ª DataNode æœåŠ¡å™¨ä¸Šã€‚
- ï¼ˆ4ï¼‰NameNode è¿”å›3ä¸ª DataNodeèŠ‚ç‚¹ï¼Œåˆ†åˆ«ä¸º dn1ã€dn2ã€dn3ã€‚
- ï¼ˆ5ï¼‰å®¢æˆ·ç«¯é€šè¿‡ FSDataOutputStream æ¨¡å—è¯·æ±‚ dn1ä¸Šä¼ æ•°æ®ï¼Œdn1 æ”¶åˆ°è¯·æ±‚ä¼šç»§ç»­è°ƒç”¨ dn2ï¼Œç„¶å dn2è°ƒç”¨ dn3ï¼Œå°†è¿™ä¸ªé€šä¿¡ç®¡é“å»ºç«‹å®Œæˆã€‚
- ï¼ˆ6ï¼‰dn1ã€dn2ã€dn3é€çº§åº”ç­”å®¢æˆ·ç«¯ã€‚
- ï¼ˆ7ï¼‰å®¢æˆ·ç«¯å¼€å§‹å¾€dn1ä¸Šä¼ ç¬¬ä¸€ä¸ª Blockï¼ˆå…ˆä»ç£ç›˜è¯»å–æ•°æ®æ”¾åˆ°ä¸€ä¸ªæœ¬åœ°å†…å­˜ç¼“å­˜ï¼‰ï¼Œä»¥ Packet ä¸ºå•ä½ï¼Œdn1æ”¶åˆ°ä¸€ä¸ªPacketå°±ä¼šä¼ ç»™dn2ï¼Œdn2ä¼ ç»™dn3ï¼›dn1æ¯ä¼ ä¸€ä¸ªpacketä¼šæ”¾å…¥ä¸€ä¸ªåº”ç­”é˜Ÿåˆ—ç­‰å¾…åº”ç­”ã€‚
- ï¼ˆ8ï¼‰å½“ä¸€ä¸ª Blockä¼ è¾“å®Œæˆä¹‹åï¼Œå®¢æˆ·ç«¯å†æ¬¡è¯·æ±‚ NameNode ä¸Šä¼ ç¬¬äºŒä¸ª Block çš„æœåŠ¡å™¨ã€‚ï¼ˆé‡å¤æ‰§è¡Œ3-7æ­¥ï¼‰ã€‚





#### ç½‘ç»œæ‹“æ‰‘-èŠ‚ç‚¹è·ç¦»è®¡ç®—



#### æœºæ¶æ„ŸçŸ¥ï¼ˆå‰¯æœ¬å­˜å‚¨èŠ‚ç‚¹é€‰æ‹©ï¼‰





### 8.2HDFS è¯»æ•°æ®æµç¨‹









## 9 NameNodeå’ŒSecondaryNameNode

## 10 DataNode

### 10.1DataNodeå·¥ä½œæœºåˆ¶

- ä¸€ä¸ªæ•°æ®å—åœ¨ DataNode ä¸Šä»¥æ–‡ä»¶å½¢å¼å­˜å‚¨åœ¨ç£ç›˜ä¸Šï¼ŒåŒ…æ‹¬ä¸¤ä¸ªæ–‡ä»¶ï¼Œä¸€ä¸ªæ˜¯æ•°æ®æœ¬èº«ï¼Œä¸€ä¸ªæ˜¯å…ƒæ•°æ®åŒ…æ‹¬æ•°æ®å—çš„é•¿åº¦ï¼Œå—æ•°æ®çš„æ ¡éªŒå’Œï¼Œä»¥åŠæ—¶é—´æˆ³ã€‚

- DataNode å¯åŠ¨åå‘ NameNode æ³¨å†Œï¼Œé€šè¿‡åï¼Œå‘¨æœŸæ€§ï¼ˆ6å°æ—¶ï¼‰çš„å‘NameNodeä¸ŠæŠ¥æ‰€æœ‰çš„å—ä¿¡æ¯ã€‚

  - DNå‘NNæ±‡æŠ¥å½“å‰è§£è¯»ä¿¡æ¯çš„æ—¶é—´é—´éš”ï¼Œé»˜è®¤6å°æ—¶

  ```xml
  <property>
  	<name>dfs.blockreport.intervalMsec</name>
  	<value>21600000</value>
  	<description>Determines block reporting interval in milliseconds.</description>
  </property>
  ```

  - DNæ‰«æè‡ªå·±èŠ‚ç‚¹å—ä¿¡æ¯åˆ—è¡¨çš„æ—¶é—´ï¼Œé»˜è®¤6å°æ—¶

  ```xml
  <property>
  	<name>dfs.datanode.directoryscan.interval</name>
  	<value>21600s</value>
  	<description>Interval in seconds for Datanode to scan data directories and reconcile the difference between blocks in memory and on the disk.
  	Support multiple time unit suffix(case insensitive), as described
  	in dfs.heartbeat.interval.
  	</description>
  </property>
  ```

  

- å¿ƒè·³æ˜¯æ¯ 3ç§’ä¸€æ¬¡ï¼Œå¿ƒè·³è¿”å›ç»“æœå¸¦æœ‰ NameNodeç»™è¯¥ DataNodeçš„å‘½ä»¤å¦‚å¤åˆ¶å—æ•°æ®åˆ°å¦ä¸€å°æœºå™¨ï¼Œæˆ–åˆ é™¤æŸä¸ªæ•°æ®å—ã€‚å¦‚æœè¶…è¿‡ 10åˆ†é’Ÿæ²¡æœ‰æ”¶åˆ°æŸä¸ª DataNodeçš„å¿ƒè·³ï¼Œåˆ™è®¤ä¸ºè¯¥èŠ‚ç‚¹ä¸å¯ç”¨ã€‚
- é›†ç¾¤è¿è¡Œä¸­å¯ä»¥å®‰å…¨åŠ å…¥å’Œé€€å‡ºä¸€äº›æœºå™¨ã€‚

### 10.2æ•°æ®å®Œæ•´æ€§

- DataNode èŠ‚ç‚¹ä¿è¯æ•°æ®å®Œæ•´æ€§çš„æ–¹æ³•ã€‚

  - ï¼ˆ1ï¼‰å½“ DataNode è¯»å– Block çš„æ—¶å€™ï¼Œå®ƒä¼šè®¡ç®— CheckSumã€‚

  - ï¼ˆ2ï¼‰å¦‚æœè®¡ç®—åçš„CheckSumï¼Œä¸ Blockåˆ›å»ºæ—¶å€¼ä¸ä¸€æ ·ï¼Œè¯´æ˜ Blockå·²ç»æŸåã€‚

  - ï¼ˆ3ï¼‰Client è¯»å–å…¶ä»– DataNodeä¸Šçš„ Blockã€‚

  - ï¼ˆ4ï¼‰å¸¸è§çš„æ ¡éªŒç®—æ³• crcï¼ˆ32ï¼‰ï¼Œmd5ï¼ˆ128ï¼‰ï¼Œsha1ï¼ˆ160ï¼‰

  - ï¼ˆ5ï¼‰DataNode åœ¨å…¶æ–‡ä»¶åˆ›å»ºåå‘¨æœŸéªŒè¯ CheckSumã€‚



### 10.3æ‰çº¿æ—¶é™å‚æ•°è®¾ç½®



# MapReduce

## 11 MapReduce æ¦‚è¿°

### 11.1MapReduce å®šä¹‰

- **åˆ†å¸ƒå¼è¿ç®—ç¨‹åº**çš„ç¼–ç¨‹æ¡†æ¶ï¼Œæ˜¯ç”¨æˆ·å¼€å‘â€œåŸºäºHadoopçš„æ•°æ®åˆ†æåº”ç”¨â€çš„æ ¸å¿ƒæ¡†æ¶ã€‚

- æ ¸å¿ƒåŠŸèƒ½æ˜¯å°†**ç”¨æˆ·ç¼–å†™çš„ä¸šåŠ¡é€»è¾‘ä»£ç **å’Œ**è‡ªå¸¦é»˜è®¤ç»„ä»¶**æ•´åˆæˆä¸€ä¸ªå®Œæ•´çš„åˆ†å¸ƒå¼è¿ç®—ç¨‹åºï¼Œå¹¶å‘è¿è¡Œåœ¨ä¸€ä¸ªHadoopé›†ç¾¤ä¸Šã€‚

### 11.2MapReduce ä¼˜ç¼ºç‚¹

#### ä¼˜ç‚¹ğŸ‘‰

- æ˜“äºç¼–ç¨‹
  - ç®€å•çš„å®ç°ä¸€äº›æ¥å£ï¼Œå°±å¯ä»¥å®Œæˆä¸€ä¸ªåˆ†å¸ƒå¼ç¨‹åºï¼Œè¿™ä¸ªåˆ†å¸ƒå¼ç¨‹åºå¯ä»¥åˆ†å¸ƒåˆ°å¤§é‡å»‰ä»·çš„PCæœºå™¨ä¸Šè¿è¡Œã€‚
- è‰¯å¥½çš„æ‰©å±•æ€§
  - å¯ä»¥é€šè¿‡ç®€å•çš„å¢åŠ æœºå™¨æ¥æ‰©å±•å®ƒçš„è®¡ç®—èƒ½åŠ›ã€‚
- é«˜å®¹é”™æ€§
  - æ¯”å¦‚**å…¶ä¸­ä¸€å°æœºå™¨æŒ‚äº†ï¼Œå®ƒå¯ä»¥æŠŠä¸Šé¢çš„è®¡ç®—ä»»åŠ¡è½¬ç§»åˆ°å¦å¤–ä¸€ä¸ªèŠ‚ç‚¹ä¸Šè¿è¡Œï¼Œä¸è‡³äºè¿™ä¸ªä»»åŠ¡è¿è¡Œå¤±è´¥**ï¼Œè€Œä¸”è¿™ä¸ªè¿‡ç¨‹ä¸éœ€è¦äººå·¥å‚ä¸ï¼Œè€Œå®Œå…¨æ˜¯ç”±Hadoopå†…éƒ¨å®Œæˆçš„ã€‚
- é€‚åˆ PBçº§ä»¥ä¸Šæµ·é‡æ•°æ®çš„ç¦»çº¿å¤„ç†
  - å¯ä»¥å®ç°ä¸Šåƒå°æœåŠ¡å™¨é›†ç¾¤å¹¶å‘å·¥ä½œï¼Œæä¾›æ•°æ®å¤„ç†èƒ½åŠ›ã€‚



#### ç¼ºç‚¹

- ä¸æ“…é•¿å®æ—¶è®¡ç®—
  - æ— æ³•åƒMySQLä¸€æ ·ï¼Œåœ¨æ¯«ç§’æˆ–è€…ç§’çº§å†…è¿”å›ç»“æœã€‚
- ä¸æ“…é•¿æµå¼è®¡ç®—
  - æµå¼è®¡ç®—çš„è¾“å…¥æ•°æ®æ˜¯åŠ¨æ€çš„ï¼Œè€Œ MapReduceçš„è¾“å…¥æ•°æ®é›†æ˜¯é™æ€çš„ï¼Œä¸èƒ½åŠ¨æ€å˜åŒ–ã€‚
  - è¿™æ˜¯å› ä¸ºMapReduceè‡ªèº«çš„è®¾è®¡ç‰¹ç‚¹å†³å®šäº†æ•°æ®æºå¿…é¡»æ˜¯é™æ€çš„ã€‚
- ä¸æ“…é•¿DAGè®¡ç®—



### 11.3MapReduce æ ¸å¿ƒæ€æƒ³



- åˆ†å¸ƒå¼çš„è¿ç®—ç¨‹åºéœ€è¦åˆ†æˆè‡³å°‘2ä¸ªé˜¶æ®µã€‚
- ç¬¬ä¸€é˜¶æ®µçš„ `MapTask` å¹¶å‘å®ä¾‹ï¼Œå®Œå…¨å¹¶è¡Œè¿è¡Œï¼Œäº’ä¸ç›¸å¹²ã€‚
- ç¬¬äºŒé˜¶æ®µçš„ `ReduceTask` å¹¶å‘å®ä¾‹äº’ä¸ç›¸å¹²ï¼Œä½†æ˜¯ä»–ä»¬çš„æ•°æ®ä¾èµ–äºä¸Šä¸€ä¸ªé˜¶æ®µçš„æ‰€æœ‰ MapTaskå¹¶å‘å®ä¾‹çš„è¾“å‡ºã€‚
- MapReduce ç¼–ç¨‹æ¨¡å‹åªèƒ½åŒ…å«ä¸€ä¸ª Map é˜¶æ®µå’Œä¸€ä¸ª Reduce é˜¶æ®µï¼Œå¦‚æœç”¨æˆ·çš„ä¸šåŠ¡é€»è¾‘éå¸¸å¤æ‚ï¼Œé‚£å°±åªèƒ½å¤šä¸ªMapReduceç¨‹åºï¼Œä¸²è¡Œè¿è¡Œã€‚
- æ€»ç»“ï¼šåˆ†æ WordCount æ•°æ®æµèµ°å‘æ·±å…¥ç†è§£ MapReduce æ ¸å¿ƒæ€æƒ³ã€‚



### 11.4MapReduce è¿›ç¨‹

- ä¸€ä¸ªå®Œæ•´çš„MapReduceç¨‹åºåœ¨åˆ†å¸ƒå¼è¿è¡Œæ—¶æœ‰ä¸‰ç±»å®ä¾‹è¿›ç¨‹
  - MrAppMasterï¼šè´Ÿè´£æ•´ä¸ªç¨‹åºçš„è¿‡ç¨‹è°ƒåº¦åŠçŠ¶æ€åè°ƒã€‚
  - MapTaskï¼šè´Ÿè´£ Map é˜¶æ®µçš„æ•´ä¸ªæ•°æ®å¤„ç†æµç¨‹ã€‚
  - ReduceTaskï¼šè´Ÿè´£ Reduce é˜¶æ®µçš„æ•´ä¸ªæ•°æ®å¤„ç†æµç¨‹ã€‚



### 11.5å®˜æ–¹WordCountæºç 

é‡‡ç”¨åç¼–è¯‘å·¥å…·åç¼–è¯‘æºç ï¼Œå‘ç° WordCountæ¡ˆä¾‹æœ‰ Mapç±»ã€Reduceç±»å’Œé©±åŠ¨ç±»ã€‚ä¸”æ•°æ®çš„ç±»å‹æ˜¯Hadoopè‡ªèº«å°è£…çš„åºåˆ—åŒ–ç±»å‹ã€‚

### 11.6å¸¸ç”¨æ•°æ®åºåˆ—åŒ–ç±»å‹

| **Javaç±»å‹** | **Hadoop Writableç±»å‹** |
| ------------ | ----------------------- |
| Boolean      | BooleanWritable         |
| Byte         | ByteWritable            |
| Int          | IntWritable             |
| Float        | FloatWritable           |
| Long         | LongWritable            |
| Double       | DoubleWritable          |
| String       | Text                    |
| Map          | MapWritable             |
| Array        | ArrayWritable           |
| Null         | NullWritable            |

###  11.7MapReduce ç¼–ç¨‹è§„èŒƒ

ç”¨æˆ·ç¼–å†™çš„ç¨‹åºåˆ†æˆä¸‰ä¸ªéƒ¨åˆ†ï¼šMapperã€Reducerå’ŒDriverã€‚

#### Mapper

- ç”¨æˆ·è‡ªå®šä¹‰çš„ Mapperè¦ç»§æ‰¿è‡ªå·±çš„çˆ¶ç±»
- Mapperçš„è¾“å…¥æ•°æ®æ˜¯ KVå¯¹çš„å½¢å¼(KVçš„ç±»å‹å¯è‡ªå®šä¹‰)
- Mapperä¸­çš„ä¸šåŠ¡é€»è¾‘å†™åœ¨ mapOæ–¹æ³•ä¸­
- Mapperçš„è¾“å‡ºæ•°æ®æ˜¯ KVå¯¹çš„å½¢å¼(KVçš„ç±»å‹å¯è‡ªå®šä¹‰)
- map0 æ–¹æ³•(Map Taskiè¿›ç¨‹)å¯¹æ¯ä¸€ä¸ª <K,V> è°ƒç”¨ä¸€æ¬¡



#### Reducer





#### Driver



### 11.8WordCountæ¡ˆä¾‹å®æ“

- éœ€æ±‚æè¿°

  - åœ¨ç»™å®šçš„æ–‡æœ¬æ–‡ä»¶ä¸­ç»Ÿè®¡è¾“å‡ºæ¯ä¸€ä¸ªå•è¯å‡ºç°çš„æ€»æ¬¡æ•°

- éœ€æ±‚åˆ†æ

  - æŒ‰ç…§ MapReduceç¼–ç¨‹è§„èŒƒï¼Œåˆ†åˆ«ç¼–å†™ Mapperï¼ŒReducerï¼ŒDriverã€‚

- ç¯å¢ƒå‡†å¤‡

  - åˆ›å»ºmavenå·¥ç¨‹ï¼ŒMapReduceDemo
  - åœ¨pom.xml æ–‡ä»¶ä¸­æ·»åŠ å¦‚ä¸‹ä¾èµ–

  ```xml
  <dependencies>
      <dependency>
          <groupId>org.apache.hadoop</groupId>
          <artifactId>hadoop-client</artifactId>
          <version>3.1.3</version>
      </dependency>
      <dependency>
          <groupId>junit</groupId>
          <artifactId>junit</artifactId>
          <version>4.12</version>
      </dependency>
      <dependency>
          <groupId>org.slf4j</groupId>
          <artifactId>slf4j-log4j12</artifactId>
          <version>1.7.30</version>
      </dependency>
  </dependencies>
  ```

  - åœ¨é¡¹ç›®çš„ src/main/resources ç›®å½•ä¸‹ï¼Œæ–°å»ºä¸€ä¸ªæ–‡ä»¶ï¼Œå‘½åä¸º â€œlog4j.propertiesâ€ï¼Œåœ¨æ–‡ä»¶ä¸­å¡«å…¥

  ```properties
  log4j.rootLogger=INFO, stdout  
  log4j.appender.stdout=org.apache.log4j.ConsoleAppender  
  log4j.appender.stdout.layout=org.apache.log4j.PatternLayout  
  log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n  
  log4j.appender.logfile=org.apache.log4j.FileAppender  
  log4j.appender.logfile.File=target/spring.log  
  log4j.appender.logfile.layout=org.apache.log4j.PatternLayout  
  log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n
  ```

  - åˆ›å»ºåŒ…åï¼šcom.atguigu.mapreduce.wordcount

- ç¼–å†™ç¨‹åº

  - ç¼–å†™ Mapper ç±»

  ```java
  package com.atguigu.mapreduce.wordcount;
  import java.io.IOException;
  import org.apache.hadoop.io.IntWritable;
  import org.apache.hadoop.io.LongWritable;
  import org.apache.hadoop.io.Text;
  import org.apache.hadoop.mapreduce.Mapper;
  
  public class WordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable>{
  	
  	Text k = new Text();
  	IntWritable v = new IntWritable(1);
  	
  	@Override
  	protected void map(LongWritable key, Text value, Context context)	throws IOException, InterruptedException {
  		
  		// 1 è·å–ä¸€è¡Œ
  		String line = value.toString();
  		
  		// 2 åˆ‡å‰²
  		String[] words = line.split(" ");
  		
  		// 3 è¾“å‡º
  		for (String word : words) {
  			
  			k.set(word);
  			context.write(k, v);
  		}
  	}
  }
  ```

  - ç¼–å†™ Reducer ç±»

  ```java
  package com.atguigu.mapreduce.wordcount;
  import java.io.IOException;
  import org.apache.hadoop.io.IntWritable;
  import org.apache.hadoop.io.Text;
  import org.apache.hadoop.mapreduce.Reducer;
  
  public class WordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable>{
  
  int sum;
  IntWritable v = new IntWritable();
  
  	@Override
  	protected void reduce(Text key, Iterable<IntWritable> values,Context context) throws IOException, InterruptedException {
  		
  		// 1 ç´¯åŠ æ±‚å’Œ
  		sum = 0;
  		for (IntWritable count : values) {
  			sum += count.get();
  		}
  		
  		// 2 è¾“å‡º
           v.set(sum);
  		context.write(key,v);
  	}
  }
  ```

  - ç¼–å†™ Driver é©±åŠ¨ç±»

  ```java
  package com.atguigu.mapreduce.wordcount;
  import java.io.IOException;
  import org.apache.hadoop.conf.Configuration;
  import org.apache.hadoop.fs.Path;
  import org.apache.hadoop.io.IntWritable;
  import org.apache.hadoop.io.Text;
  import org.apache.hadoop.mapreduce.Job;
  import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
  import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
  
  public class WordCountDriver {
  
  	public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
  
  		// 1 è·å–é…ç½®ä¿¡æ¯ä»¥åŠè·å–jobå¯¹è±¡
  		Configuration conf = new Configuration();
  		Job job = Job.getInstance(conf);
  
  		// 2 å…³è”æœ¬Driverç¨‹åºçš„jar
  		job.setJarByClass(WordCountDriver.class);
  
  		// 3 å…³è”Mapperå’ŒReducerçš„jar
  		job.setMapperClass(WordCountMapper.class);
  		job.setReducerClass(WordCountReducer.class);
  
  		// 4 è®¾ç½®Mapperè¾“å‡ºçš„kvç±»å‹
  		job.setMapOutputKeyClass(Text.class);
  		job.setMapOutputValueClass(IntWritable.class);
  
  		// 5 è®¾ç½®æœ€ç»ˆè¾“å‡ºkvç±»å‹
  		job.setOutputKeyClass(Text.class);
  		job.setOutputValueClass(IntWritable.class);
  		
  		// 6 è®¾ç½®è¾“å…¥å’Œè¾“å‡ºè·¯å¾„
  		FileInputFormat.setInputPaths(job, new Path(args[0]));
  		FileOutputFormat.setOutputPath(job, new Path(args[1]));
  
  		// 7 æäº¤job
  		boolean result = job.waitForCompletion(true);
  		System.exit(result ? 0 : 1);
  	}
  }
  ```

  

- æœ¬åœ°æµ‹è¯•

  - éœ€è¦é¦–å…ˆé…ç½®å¥½ HADOOP_HOME å˜é‡ä»¥åŠ Windows è¿è¡Œä¾èµ–
  - åœ¨ IDEA/Eclipse ä¸Šè¿è¡Œç¨‹åº



## 12 Hadoop åºåˆ—åŒ–

### 12.1åºåˆ—åŒ–æ¦‚è¿°

- ä»€ä¹ˆæ˜¯åºåˆ—åŒ–
  - åºåˆ—åŒ–å°±æ˜¯**æŠŠå†…å­˜ä¸­çš„å¯¹è±¡ï¼Œè½¬æ¢æˆå­—èŠ‚åºåˆ—**ï¼ˆæˆ–å…¶ä»–æ•°æ®ä¼ è¾“åè®®ï¼‰ä»¥ä¾¿äºå­˜å‚¨åˆ°ç£ç›˜ï¼ˆæŒä¹…åŒ–ï¼‰å’Œç½‘ç»œä¼ è¾“ã€‚
  - ååºåˆ—åŒ–å°±æ˜¯å°†æ”¶åˆ°å­—èŠ‚åºåˆ—ï¼ˆæˆ–å…¶ä»–æ•°æ®ä¼ è¾“åè®®ï¼‰æˆ–è€…æ˜¯**ç£ç›˜çš„æŒä¹…åŒ–æ•°æ®ï¼Œè½¬æ¢æˆå†…å­˜ä¸­çš„å¯¹è±¡**ã€‚

- ä¸ºä»€ä¹ˆè¦åºåˆ—åŒ–
  - ä¸€èˆ¬æ¥è¯´ï¼Œâ€œæ´»çš„â€å¯¹è±¡åªç”Ÿå­˜åœ¨å†…å­˜é‡Œï¼Œå…³æœºæ–­ç”µå°±æ²¡æœ‰äº†ã€‚è€Œä¸”â€œæ´»çš„â€å¯¹è±¡åªèƒ½ç”±æœ¬åœ°çš„è¿›ç¨‹ä½¿ç”¨ï¼Œä¸èƒ½è¢«å‘é€åˆ°ç½‘ç»œä¸Šçš„å¦å¤–ä¸€å°è®¡ç®—æœºã€‚
  - **ç„¶è€Œåºåˆ—åŒ–å¯ä»¥å­˜å‚¨â€œæ´»çš„â€å¯¹è±¡ï¼Œå¯ä»¥å°†â€œæ´»çš„â€å¯¹è±¡å‘é€åˆ°è¿œç¨‹è®¡ç®—æœº**ã€‚
- ä¸ºä»€ä¹ˆä¸ç”¨ Java çš„åºåˆ—åŒ–
  - `Java` çš„åºåˆ—åŒ–æ˜¯ä¸€ä¸ª**é‡é‡çº§åºåˆ—åŒ–æ¡†æ¶**ï¼ˆSerializableï¼‰ï¼Œä¸€ä¸ªå¯¹è±¡è¢«åºåˆ—åŒ–åï¼Œä¼šé™„å¸¦å¾ˆå¤šé¢å¤–çš„ä¿¡æ¯ï¼ˆå„ç§æ ¡éªŒä¿¡æ¯ï¼ŒHeaderï¼Œç»§æ‰¿ä½“ç³»ç­‰ï¼‰ï¼Œä¸ä¾¿äºåœ¨ç½‘ç»œä¸­é«˜æ•ˆä¼ è¾“ã€‚
  - æ‰€ä»¥ï¼ŒHadoopè‡ªå·±å¼€å‘äº†ä¸€å¥—åºåˆ—åŒ–æœºåˆ¶ï¼ˆWritableï¼‰ã€‚
- Hadoop åºåˆ—åŒ–ç‰¹ç‚¹
  - ç´§å‡‘ï¼šé«˜æ•ˆä½¿ç”¨å­˜å‚¨ç©ºé—´
  - å¿«é€Ÿï¼šè¯»å†™æ•°æ®çš„é¢å¤–å¼€é”€å°
  - äº’æ“ä½œï¼šæ”¯æŒå¤šè¯­è¨€çš„äº¤äº’



### 12.2è‡ªå®šä¹‰beanå¯¹è±¡å®ç°åºåˆ—åŒ–æ¥å£ï¼ˆWritableï¼‰

- åœ¨ä¼ä¸šå¼€å‘ä¸­å¾€å¾€å¸¸ç”¨çš„åŸºæœ¬åºåˆ—åŒ–ç±»å‹ä¸èƒ½æ»¡è¶³æ‰€æœ‰éœ€æ±‚ï¼Œæ¯”å¦‚åœ¨Hadoopæ¡†æ¶å†…éƒ¨ä¼ é€’ä¸€ä¸ª `bean` å¯¹è±¡ï¼Œé‚£ä¹ˆè¯¥å¯¹è±¡å°±éœ€è¦å®ç°åºåˆ—åŒ–æ¥å£ã€‚

- å…·ä½“å®ç° bean å¯¹è±¡åºåˆ—åŒ–æ­¥éª¤å¦‚ä¸‹7æ­¥ã€‚

  - ï¼ˆ1ï¼‰å¿…é¡»å®ç° `Writable` æ¥å£
  - ï¼ˆ2ï¼‰ååºåˆ—åŒ–æ—¶ï¼Œéœ€è¦åå°„è°ƒç”¨ç©ºå‚æ„é€ å‡½æ•°ï¼Œæ‰€ä»¥å¿…é¡»æœ‰ç©ºå‚æ„é€ 

  ```java
  public FlowBean() {
  	super();
  }
  ```

  - ï¼ˆ3ï¼‰é‡å†™åºåˆ—åŒ–æ–¹æ³•

  ```java
  @Override
  public void write(DataOutput out) throws IOException {
  	out.writeLong(upFlow);
  	out.writeLong(downFlow);
  	out.writeLong(sumFlow);
  }
  ```

  - ï¼ˆ4ï¼‰é‡å†™ååºåˆ—åŒ–æ–¹æ³•

  ```java
  @Override
  public void readFields(DataInput in) throws IOException {
  	upFlow = in.readLong();
  	downFlow = in.readLong();
  	sumFlow = in.readLong();
  }
  ```

  - ï¼ˆ5ï¼‰æ³¨æ„ååºåˆ—åŒ–çš„é¡ºåºå’Œåºåˆ—åŒ–çš„é¡ºåºå®Œå…¨ä¸€è‡´
  - ï¼ˆ6ï¼‰è¦æƒ³æŠŠç»“æœæ˜¾ç¤ºåœ¨æ–‡ä»¶ä¸­ï¼Œéœ€è¦é‡å†™ `toString()` ï¼Œå¯ç”¨ "\t" åˆ†å¼€ï¼Œæ–¹ä¾¿åç»­ç”¨ã€‚
  - ï¼ˆ7ï¼‰å¦‚æœéœ€è¦å°†è‡ªå®šä¹‰çš„ bean æ”¾åœ¨ key ä¸­ä¼ è¾“ï¼Œåˆ™è¿˜éœ€è¦å®ç° `Comparable` æ¥å£ï¼Œå› ä¸ºMapReduceæ¡†ä¸­çš„ Shuffle è¿‡ç¨‹è¦æ±‚å¯¹ key å¿…é¡»èƒ½æ’åºã€‚è¯¦è§åé¢æ’åºæ¡ˆä¾‹ã€‚

### 12.3åºåˆ—åŒ–æ¡ˆä¾‹å®æ“

- éœ€æ±‚
- éœ€æ±‚åˆ†æ
- ç¼–å†™ `MapReduce` ç¨‹åº

  - ç¼–å†™æµé‡ç»Ÿè®¡çš„ Beanå¯¹è±¡

  ```java
  package com.atguigu.mapreduce.writable;
  
  import org.apache.hadoop.io.Writable;
  import java.io.DataInput;
  import java.io.DataOutput;
  import java.io.IOException;
  
  //1 ç»§æ‰¿Writableæ¥å£
  public class FlowBean implements Writable {
  
      private long upFlow; //ä¸Šè¡Œæµé‡
      private long downFlow; //ä¸‹è¡Œæµé‡
      private long sumFlow; //æ€»æµé‡
  
      //2 æä¾›æ— å‚æ„é€ 
      public FlowBean() {
      }
  
      //3 æä¾›ä¸‰ä¸ªå‚æ•°çš„getterå’Œsetteræ–¹æ³•
      public long getUpFlow() {
          return upFlow;
      }
      public void setUpFlow(long upFlow) {
          this.upFlow = upFlow;
      }
      
      public long getDownFlow() {
          return downFlow;
      }
      public void setDownFlow(long downFlow) {
          this.downFlow = downFlow;
      }
      
      public long getSumFlow() {
          return sumFlow;
      }
      public void setSumFlow(long sumFlow) {
          this.sumFlow = sumFlow;
      }
  
      public void setSumFlow() {
          this.sumFlow = this.upFlow + this.downFlow;
      }
  
      //4 å®ç°åºåˆ—åŒ–å’Œååºåˆ—åŒ–æ–¹æ³•,æ³¨æ„é¡ºåºä¸€å®šè¦ä¿æŒä¸€è‡´
      @Override
      public void write(DataOutput dataOutput) throws IOException {
          dataOutput.writeLong(upFlow);
          dataOutput.writeLong(downFlow);
          dataOutput.writeLong(sumFlow);
      }
  
      @Override
      public void readFields(DataInput dataInput) throws IOException {
          this.upFlow = dataInput.readLong();
          this.downFlow = dataInput.readLong();
          this.sumFlow = dataInput.readLong();
      }
  
      //5 é‡å†™ToString
      @Override
      public String toString() {
          return upFlow + "\t" + downFlow + "\t" + sumFlow;
      }
  }
  ```

  - ç¼–å†™Mapperç±»

  ```java
  package com.atguigu.mapreduce.writable;
  
  import org.apache.hadoop.io.LongWritable;
  import org.apache.hadoop.io.Text;
  import org.apache.hadoop.mapreduce.Mapper;
  import java.io.IOException;
  
  public class FlowMapper extends Mapper<LongWritable, Text, Text, FlowBean> {
      private Text outK = new Text();
      private FlowBean outV = new FlowBean();
  
      @Override
      protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
  
          //1 è·å–ä¸€è¡Œæ•°æ®,è½¬æˆå­—ç¬¦ä¸²
          String line = value.toString();
  
          //2 åˆ‡å‰²æ•°æ®
          String[] split = line.split("\t");
  
          //3 æŠ“å–æˆ‘ä»¬éœ€è¦çš„æ•°æ®:æ‰‹æœºå·,ä¸Šè¡Œæµé‡,ä¸‹è¡Œæµé‡
          String phone = split[1];
          String up = split[split.length - 3];
          String down = split[split.length - 2];
  
          //4 å°è£…outK outV
          outK.set(phone);
          outV.setUpFlow(Long.parseLong(up));
          outV.setDownFlow(Long.parseLong(down));
          outV.setSumFlow();
  
          //5 å†™å‡ºoutK outV
          context.write(outK, outV);
      }
  }
  ```

  - ç¼–å†™ Reducerç±»

  ```java
  package com.atguigu.mapreduce.writable;
  
  import org.apache.hadoop.io.Text;
  import org.apache.hadoop.mapreduce.Reducer;
  import java.io.IOException;
  
  public class FlowReducer extends Reducer<Text, FlowBean, Text, FlowBean> {
      private FlowBean outV = new FlowBean();
      @Override
      protected void reduce(Text key, Iterable<FlowBean> values, Context context) throws IOException, InterruptedException {
  
          long totalUp = 0;
          long totalDown = 0;
  
          //1 éå†values,å°†å…¶ä¸­çš„ä¸Šè¡Œæµé‡,ä¸‹è¡Œæµé‡åˆ†åˆ«ç´¯åŠ 
          for (FlowBean flowBean : values) {
              totalUp += flowBean.getUpFlow();
              totalDown += flowBean.getDownFlow();
          }
  
          //2 å°è£…outKV
          outV.setUpFlow(totalUp);
          outV.setDownFlow(totalDown);
          outV.setSumFlow();
  
          //3 å†™å‡ºoutK outV
          context.write(key,outV);
      }
  }
  ```

  - ç¼–å†™Driveré©±åŠ¨ç±»

  ```java
  package com.atguigu.mapreduce.writable;
  
  import org.apache.hadoop.conf.Configuration;
  import org.apache.hadoop.fs.Path;
  import org.apache.hadoop.io.Text;
  import org.apache.hadoop.mapreduce.Job;
  import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
  import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
  import java.io.IOException;
  
  public class FlowDriver {
      public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
  
          //1 è·å– jobå¯¹è±¡
          Configuration conf = new Configuration();
          Job job = Job.getInstance(conf);
  
          //2 å…³è”æœ¬ Driverç±»
          job.setJarByClass(FlowDriver.class);
  
          //3 å…³è” Mapperå’ŒReducer
          job.setMapperClass(FlowMapper.class);
          job.setReducerClass(FlowReducer.class);
          
  		//4 è®¾ç½®Mapç«¯è¾“å‡ºKVç±»å‹
          job.setMapOutputKeyClass(Text.class);
          job.setMapOutputValueClass(FlowBean.class);
          
  		//5 è®¾ç½®ç¨‹åºæœ€ç»ˆè¾“å‡ºçš„KVç±»å‹
          job.setOutputKeyClass(Text.class);
          job.setOutputValueClass(FlowBean.class);
          
  		//6 è®¾ç½®ç¨‹åºçš„è¾“å…¥è¾“å‡ºè·¯å¾„
          FileInputFormat.setInputPaths(job, new Path("D:\\inputflow"));
          FileOutputFormat.setOutputPath(job, new Path("D:\\flowoutput"));
          
  		//7 æäº¤Job
          boolean b = job.waitForCompletion(true);
          System.exit(b ? 0 : 1);
      }
  }
  ```

  

## 13 MapReduceæ¡†æ¶åŸç†

### 13.1 InputFormatæ•°æ®è¾“å…¥

### 13.2 å·¥ä½œæµç¨‹

### 13.3 Shuffleæœºåˆ¶

### 13.4 OutputFormatæ•°æ®è¾“å‡º

### 13.5 MapReduceå†…æ ¸æºç è§£æ

### 13.6 Joinåº”ç”¨

### 13.7 æ•°æ®æ¸…æ´—ï¼ˆETLï¼‰

### 13.8 MapReduceå¼€å‘æ€»ç»“





## 14 Hadoopæ•°æ®å‹ç¼©

### 14.1 æ¦‚è¿°

### 14.2 MRæ”¯æŒçš„å‹ç¼©ç¼–ç 

- å‹ç¼©ç®—æ³•å¯¹æ¯”ä»‹ç»

| å‹ç¼©æ ¼å¼ | Hadoopè‡ªå¸¦ï¼Ÿ | ç®—æ³•    | æ–‡ä»¶æ‰©å±•å | æ˜¯å¦å¯åˆ‡ç‰‡ | æ¢æˆå‹ç¼©æ ¼å¼åï¼ŒåŸæ¥çš„ç¨‹åºæ˜¯å¦éœ€è¦ä¿®æ”¹ |
| -------- | ------------ | ------- | ---------- | ---------- | -------------------------------------- |
| DEFLATE  | æ˜¯ï¼Œç›´æ¥ä½¿ç”¨ | DEFLATE | .deflate   | å¦         | å’Œæ–‡æœ¬å¤„ç†ä¸€æ ·ï¼Œä¸éœ€è¦ä¿®æ”¹             |
| Gzip     | æ˜¯ï¼Œç›´æ¥ä½¿ç”¨ | DEFLATE | .gz        | å¦         | å’Œæ–‡æœ¬å¤„ç†ä¸€æ ·ï¼Œä¸éœ€è¦ä¿®æ”¹             |
| bzip2    | æ˜¯ï¼Œç›´æ¥ä½¿ç”¨ | bzip2   | .bz2       | æ˜¯         | å’Œæ–‡æœ¬å¤„ç†ä¸€æ ·ï¼Œä¸éœ€è¦ä¿®æ”¹             |
| LZO      | å¦ï¼Œéœ€è¦å®‰è£… | LZO     | .lzo       | æ˜¯         | éœ€è¦å»ºç´¢å¼•ï¼Œè¿˜éœ€è¦æŒ‡å®šè¾“å…¥æ ¼å¼         |
| Snappy   | æ˜¯ï¼Œç›´æ¥ä½¿ç”¨ | Snappy  | .snappy    | å¦         | å’Œæ–‡æœ¬å¤„ç†ä¸€æ ·ï¼Œä¸éœ€è¦ä¿®æ”¹             |

- å‹ç¼©æ€§èƒ½çš„æ¯”è¾ƒ

| å‹ç¼©ç®—æ³• | åŸå§‹æ–‡ä»¶å¤§å° | å‹ç¼©æ–‡ä»¶å¤§å° | å‹ç¼©é€Ÿåº¦ | è§£å‹é€Ÿåº¦ |
| -------- | ------------ | ------------ | -------- | -------- |
| gzip     | 8.3GB        | 1.8GB        | 17.5MB/s | 58MB/s   |
| bzip2    | 8.3GB        | 1.1GB        | 2.4MB/s  | 9.5MB/s  |
| LZO      | 8.3GB        | 2.9GB        | 49.3MB/s | 74.6MB/s |

### 14.3 å‹ç¼©æ–¹å¼é€‰æ‹©

å‹ç¼©æ–¹å¼é€‰æ‹©æ—¶é‡ç‚¹è€ƒè™‘ï¼š**å‹ç¼© / è§£å‹ç¼©é€Ÿåº¦ã€å‹ç¼©ç‡ï¼ˆå‹ç¼©åå­˜å‚¨å¤§å°ï¼‰ã€å‹ç¼©åæ˜¯å¦å¯ä»¥æ”¯æŒåˆ‡ç‰‡**ã€‚



### 14.4 å‹ç¼©å‚æ•°é…ç½®

- ä¸ºäº†æ”¯æŒå¤šç§å‹ç¼©/è§£å‹ç¼©ç®—æ³•ï¼ŒHadoopå¼•å…¥äº†ç¼–ç /è§£ç å™¨

| å‹ç¼©æ ¼å¼ | å¯¹åº”çš„ç¼–ç /è§£ç å™¨                          |
| -------- | ------------------------------------------ |
| DEFLATE  | org.apache.hadoop.io.compress.DefaultCodec |
| gzip     | org.apache.hadoop.io.compress.GzipCodec    |
| bzip2    | org.apache.hadoop.io.compress.BZip2Codec   |
| LZO      | com.hadoop.compression.lzo.LzopCodec       |
| Snappy   | org.apache.hadoop.io.compress.SnappyCodec  |

- è¦åœ¨Hadoopä¸­å¯ç”¨å‹ç¼©ï¼Œå¯ä»¥é…ç½®å¦‚ä¸‹å‚æ•°

| å‚æ•°                                                         | é»˜è®¤å€¼                                              | é˜¶æ®µ        | å»ºè®®                                          |
| ------------------------------------------------------------ | --------------------------------------------------- | ----------- | --------------------------------------------- |
| io.compression.codecs<br/>  ï¼ˆåœ¨core-site.xmlä¸­é…ç½®ï¼‰        | æ— ï¼Œè¿™ä¸ªéœ€è¦åœ¨å‘½ä»¤è¡Œè¾“å…¥<br/>hadoop checknativeæŸ¥çœ‹ | è¾“å…¥å‹ç¼©    | Hadoopä½¿ç”¨æ–‡ä»¶æ‰©å±•ååˆ¤æ–­æ˜¯å¦æ”¯æŒæŸç§ç¼–è§£ç å™¨  |
| mapreduce.map.output.compress<br>ï¼ˆåœ¨mapred-site.xmlä¸­é…ç½®ï¼‰ | false                                               | mapperè¾“å‡º  | è¿™ä¸ªå‚æ•°è®¾ä¸ºtrueå¯ç”¨å‹ç¼©                      |
| mapreduce.map.output.compress.codec<br/>ï¼ˆåœ¨mapred-site.xmlä¸­é…ç½®ï¼‰ | org.apache.hadoop.io<br/>.compress.DefaultCodec     | mapperè¾“å‡º  | ä¼ä¸šå¤šä½¿ç”¨LZOæˆ–Snappyç¼–è§£ç å™¨åœ¨æ­¤é˜¶æ®µå‹ç¼©æ•°æ® |
| mapreduce.output.fileoutputformat<br/>.compressï¼ˆåœ¨mapred-site.xmlä¸­é…ç½®ï¼‰ | false                                               | reducerè¾“å‡º | è¿™ä¸ªå‚æ•°è®¾ä¸ºtrueå¯ç”¨å‹ç¼©                      |
| mapreduce.output.fileoutputformat<br/>.compress.codecï¼ˆåœ¨mapred-site.xmlä¸­é…ç½®ï¼‰ | org.apache.hadoop.io<br/>.compress.DefaultCodec     | reducerè¾“å‡º | ä½¿ç”¨æ ‡å‡†å·¥å…·æˆ–è€…ç¼–è§£ç å™¨ï¼Œå¦‚gzipå’Œbzip2       |



### 14.5 å‹ç¼©å®æ“æ¡ˆä¾‹



## 15 å¸¸è§é”™è¯¯åŠè§£å†³æ–¹æ¡ˆ



# yarn

## 16 Yarnèµ„æºè°ƒåº¦å™¨

### 16.1åŸºç¡€æ¶æ„





### 16.2å·¥ä½œæœºåˆ¶





### 16.3ä½œä¸šæäº¤å…¨è¿‡ç¨‹





### 16.4è°ƒåº¦å™¨å’Œè°ƒåº¦ç®—æ³•



### 16.5å¸¸ç”¨å‘½ä»¤





### 16.6ç”Ÿäº§ç¯å¢ƒæ ¸å¿ƒå‚æ•°





## 17 Yarnæ¡ˆä¾‹å®æ“

> æ³¨ï¼šè°ƒæ•´ä¸‹åˆ—å‚æ•°ä¹‹å‰å°½é‡æ‹æ‘„Linuxå¿«ç…§ï¼Œå¦åˆ™åç»­çš„æ¡ˆä¾‹ï¼Œè¿˜éœ€è¦é‡å†™å‡†å¤‡é›†ç¾¤ã€‚





# æºç è§£æ













# ç”Ÿäº§è°ƒä¼˜æ‰‹å†Œ





# æ¨è

- [å°šç¡…è°·å¤§æ•°æ®Hadoopæ•™ç¨‹ï¼ˆHadoop 3.xå®‰è£…æ­å»ºåˆ°é›†ç¾¤è°ƒä¼˜ï¼‰](https://www.bilibili.com/video/BV1Qp4y1n7EN)

```bash
# 178Pï¼Œ27.5å°æ—¶

# è¿›è¡Œä¸­
```











































































![](https://notes2021.oss-cn-beijing.aliyuncs.com/2021/image-20220730085217510.png)





